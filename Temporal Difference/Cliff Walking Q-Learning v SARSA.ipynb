{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking Q-Learning v SARSA \n",
    "SARSA on-policy control v q-learning off-policy control\n",
    "\n",
    "Comparison of methods as per the example in the Sutton Barto Text.\n",
    "\n",
    "Objects to create:\n",
    "- environment to simulate the 'cliff walk'\n",
    "- entity to simulate the traversal thru the paths\n",
    "    \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "The blue line represents the expected SARSA path and the red is the expected Q-learning path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment object\n",
    "class environment():\n",
    "    def __init__(self, dims = [4,10], start = [3,0] , terminal = [3,9], reward = -1, the_cliff_reward = -100, the_cliff = [(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(3,7),(3,8)]):\n",
    "        \"\"\"\n",
    "        this class initializes the environment\n",
    "        \n",
    "        inputs\n",
    "        dims - a scalar to define a dxd grid environment\n",
    "        terminals - a list of lists of ordered pairs representing the location in the grid of the terminal states\n",
    "        reward -  the reward value for moving one space (unless in a terminal state)\n",
    "        \n",
    "        self defined variables\n",
    "        actions - determines actions that can be taken in the environment where 0 = left, 1 = up, 2 = right, 3 = down\n",
    "        \n",
    "        returns\n",
    "        None\n",
    "        \n",
    "        cheating here as i did not do checks on things for integrity...\n",
    "        \"\"\"\n",
    "        self.dims = dims\n",
    "        self.start = start\n",
    "        self.terminal = terminal\n",
    "        self.reward = reward\n",
    "        self.the_cliff_reward = the_cliff_reward\n",
    "        self.the_cliff = the_cliff\n",
    "        self.actions = [np.array([0, -1]),\n",
    "                       np.array([-1, 0]),\n",
    "                       np.array([0, 1]),\n",
    "                       np.array([1, 0])]\n",
    "            \n",
    "    def is_terminal(self,i,j):\n",
    "        \"\"\"\n",
    "        This function checks if coordinates are for a terminal state\n",
    "        \n",
    "        inputs\n",
    "        i = y coord (row)\n",
    "        j = x coord (col)\n",
    "        \n",
    "        returns\n",
    "        boolean on terminal status\n",
    "        \"\"\"\n",
    "        #print(\"episode end\")\n",
    "        return [i,j] == self.terminal\n",
    "        \n",
    "        \n",
    "    def next_state(self,i,j,a):\n",
    "        \"\"\"\n",
    "        this function determines the next state and the reward received\n",
    "        This function also checks if current state is terminal, or next state is terminal\n",
    "        This function will also bounce back to the original coordinates if it is an edge/border square in the grid\n",
    "        \n",
    "        inputs\n",
    "        i = y coord (row)\n",
    "        j = x coord (col)\n",
    "        a = action taken where 0 = left, 1 = up, 2 = right, 3 = down\n",
    "        \n",
    "        returns\n",
    "        i_next, j_next representing next state coordinates\n",
    "        reward received for moving\n",
    "        \"\"\"\n",
    "        # check if current state is terminal, if so return current state and reward = 0\n",
    "        #i = row, j = col\n",
    "        if self.is_terminal(i,j):\n",
    "            return i, j, 0\n",
    "        \n",
    "        # get next state\n",
    "        next = [i,j] + self.actions[a] \n",
    "        #print(next)\n",
    "        \n",
    "        #check if move takes off grid on top or left go back to original space\n",
    "        if next[0] < 0 or next[1] < 0:\n",
    "            return i, j, self.reward\n",
    "        #check if next is the cliff go back to start\n",
    "        elif (next[0], next[1]) in self.the_cliff:\n",
    "            return self.start[0], self.start[1], self.the_cliff_reward\n",
    "        #check if off bottom, or right go back to original space\n",
    "        elif (next[0] > (self.dims[0]-1)) or (next[1] > (self.dims[1]-1)):\n",
    "            return i, j, self.reward \n",
    "        else:\n",
    "            return next[0], next[1], self.reward \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sarsa_entity():\n",
    "    def __init__(self, state_action_values, environment, algo_param_alpha = 1, gamma = 0.8, epsilon = 0.05, start = [3,0]):\n",
    "        \"\"\"\n",
    "        This class initializes the entity which will \n",
    "        walk thru the grid world and collect data\n",
    "        this follows a puerly greedy policy atm\n",
    "        \n",
    "        inputs\n",
    "        rand_policy = policy ot follow if not initialized\n",
    "        state_action_values - calculated state-action values for all states except the terminal state\n",
    "        algo_param_alpha - step size to be used\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_action_values = state_action_values\n",
    "        self.algo_param_alpha = algo_param_alpha\n",
    "        self.gamma = gamma\n",
    "        self.environment = environment\n",
    "        self.start_location = start\n",
    "        self.current_location = start\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "                \n",
    "    def pick_action(self, state_action_values):\n",
    "        #need to fix this asap\n",
    "        \n",
    "        #simple pick action based in random policy to go any direction with equal probability\n",
    "        greedy_action = np.argmax(state_action_values)\n",
    "        #print(greedy_action)\n",
    "        \n",
    "        action = greedy_action\n",
    "        \n",
    "        if self.epsilon == 0:\n",
    "            return action\n",
    "        \n",
    "        if np.random.binomial(1, self.epsilon, 1):\n",
    "            temp_list = [0,1,2,3]\n",
    "            #print(temp_list)\n",
    "            temp_list.pop(greedy_action)\n",
    "            #print(temp_list)\n",
    "            action = np.random.choice(temp_list)\n",
    "            #print(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        next_i, next_j, reward = self.environment.next_state(self.current_location[0], self.current_location[1], action)\n",
    "        return next_i, next_j, reward\n",
    "    \n",
    "    def restart_walk(self):\n",
    "        self.current_location = self.start_location\n",
    "    \n",
    "    def take_a_walk(self):\n",
    "        \n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "        \n",
    "        #choose first action according to e-greedy\n",
    "        i, j = self.current_location[0], self.current_location[1]\n",
    "        state_action_values = self.state_action_values[(i,j)]\n",
    "        action = self.pick_action(state_action_values)\n",
    "        next_i, next_j, reward = self.take_action(action)\n",
    "        \n",
    "        while not terminal:\n",
    "\n",
    "            state_action_values = self.state_action_values[(next_i, next_j)]\n",
    "            next_action = self.pick_action(state_action_values)\n",
    "            next_action_value = state_action_values[next_action]\n",
    "            \n",
    "            #update last state value\n",
    "            self.state_action_values[(i,j)][action] = \\\n",
    "            (self.state_action_values[(i,j)][action] + \\\n",
    "            (self.algo_param_alpha * \\\n",
    "            (reward + \\\n",
    "            (self.gamma * next_action_value) - \\\n",
    "            self.state_action_values[(i,j)][action])))\n",
    "             \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j] \n",
    "            i, j = next_i, next_j\n",
    "            \n",
    "            #update to iterate \n",
    "            action = next_action\n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "            \n",
    "            \n",
    "            if reward == 0:\n",
    "                terminal = True\n",
    "                \n",
    "    def policy_walk(self):\n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "        \n",
    "        action_dict = {0 : \"left\", 1 : \"up\", 2 : \"right\", 3 : \"down\"}\n",
    "        \n",
    "        number_of_steps = 0\n",
    "        \n",
    "        orig_epsilon = self.epsilon\n",
    "        \n",
    "        self.epsilon = 0\n",
    "        \n",
    "        while not terminal:\n",
    "            print(\"current state is:\", self.current_location)\n",
    "            print(\"current step is:\", number_of_steps)\n",
    "            state_action_values = self.state_action_values[(self.current_location[0],self.current_location[1])]\n",
    "            \n",
    "            action = self.pick_action(state_action_values)\n",
    "            print(\"next action is:\", action_dict[action])\n",
    "            \n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "            \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j] \n",
    "            \n",
    "            number_of_steps = number_of_steps + 1\n",
    "            \n",
    "            if number_of_steps > 100:\n",
    "                print(\"walk failure\")\n",
    "                reward = 0\n",
    "            \n",
    "            if reward == 0:\n",
    "                print(\"you have reached the terminal state in \", number_of_steps, \" steps!\")\n",
    "                self.epsilon = orig_epsilon                      \n",
    "                terminal = True          \n",
    "                \n",
    "    def give_state_action_values(self):\n",
    "        return self.state_action_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_entity():\n",
    "    def __init__(self, state_action_values, environment, algo_param_alpha = 0.25, gamma = 0.5, epsilon = 0.05, start = [3,0]):\n",
    "        \"\"\"\n",
    "        This class initializes the entity which will \n",
    "        walk thru the grid world and collect data\n",
    "        this follows a puerly greedy policy atm\n",
    "        \n",
    "        inputs\n",
    "        rand_policy = policy ot follow if not initialized\n",
    "        state_action_values - calculated state-action values for all states except the terminal state\n",
    "        algo_param_alpha - step size to be used\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_action_values = state_action_values\n",
    "        self.algo_param_alpha = algo_param_alpha\n",
    "        self.gamma = gamma\n",
    "        self.environment = environment\n",
    "        self.start_location = start\n",
    "        self.current_location = start\n",
    "        self.epsilon = epsilon\n",
    "        self.orig_epslion = epsilon\n",
    "\n",
    "                \n",
    "    def pick_action(self, state_action_values):\n",
    "        #need to fix this asap\n",
    "        \n",
    "        #simple pick action based in random policy to go any direction with equal probability\n",
    "        greedy_action = np.argmax(state_action_values)\n",
    "        #print(greedy_action)\n",
    "        \n",
    "        action = greedy_action\n",
    "        \n",
    "        if self.epsilon == 0:\n",
    "            return action\n",
    "        \n",
    "        if np.random.binomial(1, self.epsilon, 1):\n",
    "            temp_list = [0,1,2,3]\n",
    "            #print(temp_list)\n",
    "            temp_list.pop(greedy_action)\n",
    "            #print(temp_list)\n",
    "            action = np.random.choice(temp_list)\n",
    "            #print(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        next_i, next_j, reward = self.environment.next_state(self.current_location[0], self.current_location[1], action)\n",
    "        return next_i, next_j, reward\n",
    "    \n",
    "    def restart_walk(self):\n",
    "        self.current_location = self.start_location\n",
    "    \n",
    "    def take_a_walk(self):\n",
    "        \n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "               \n",
    "        while not terminal:\n",
    "            #choose actual action being taken according to e-greedy\n",
    "            i, j = self.current_location[0], self.current_location[1]\n",
    "            state_action_values = self.state_action_values[(i,j)]\n",
    "            action = self.pick_action(state_action_values)\n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "\n",
    "            state_action_values = self.state_action_values[(next_i, next_j)]\n",
    "            \n",
    "            #flip epsilon to 0 and pick next action as a greedy choice\n",
    "            self.epsilon = 0\n",
    "            next_action = self.pick_action(state_action_values)\n",
    "            self.epsilon = self.orig_epslion\n",
    "            \n",
    "            next_action_value = state_action_values[next_action]\n",
    "            \n",
    "            #update last state value\n",
    "            self.state_action_values[(i,j)][action] = \\\n",
    "            (self.state_action_values[(i,j)][action] + \\\n",
    "            (self.algo_param_alpha * \\\n",
    "            (reward + \\\n",
    "            (self.gamma * next_action_value) - \\\n",
    "            self.state_action_values[(i,j)][action])))\n",
    "             \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j]          \n",
    "            \n",
    "            if reward == 0:\n",
    "                terminal = True\n",
    "                \n",
    "    def policy_walk(self):\n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "        \n",
    "        action_dict = {0 : \"left\", 1 : \"up\", 2 : \"right\", 3 : \"down\"}\n",
    "        \n",
    "        number_of_steps = 0\n",
    "        \n",
    "        orig_epsilon = self.epsilon\n",
    "        \n",
    "        self.epsilon = 0\n",
    "        \n",
    "        while not terminal:\n",
    "            print(\"current state is:\", self.current_location)\n",
    "            print(\"current step is:\", number_of_steps)\n",
    "            state_action_values = self.state_action_values[(self.current_location[0],self.current_location[1])]\n",
    "            \n",
    "            action = self.pick_action(state_action_values)\n",
    "            print(\"next action is:\", action_dict[action])\n",
    "            \n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "            \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j] \n",
    "            \n",
    "            number_of_steps = number_of_steps + 1\n",
    "            \n",
    "            if number_of_steps > 100:\n",
    "                print(\"walk failure\")\n",
    "                reward = 0\n",
    "            \n",
    "            if reward == 0:\n",
    "                print(\"you have reached the terminal state in \", number_of_steps, \" steps!\")\n",
    "                self.epsilon = orig_epsilon                      \n",
    "                terminal = True          \n",
    "                \n",
    "    def give_state_action_values(self):\n",
    "        return self.state_action_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step size\n",
    "algo_param_alpha = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions\n",
    "dims = [4,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arbitrary state_action_values\n",
    "#using a dict because look up is much easier then\n",
    "state_action_values1 = {(None,None,None):0}\n",
    "\n",
    "for i in range(dims[0]):\n",
    "    for j in range(dims[1]):\n",
    "        state_action_values1[(i,j)] = [0,0,0,0]\n",
    "        \n",
    "state_action_values1.pop((None,None,None))\n",
    "            \n",
    "#state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment1 = environment()\n",
    "entity1 = sarsa_entity(state_action_values1, environment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_episodes = 1000\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    #if episode%10 == 0:\n",
    "    #print(\"episode #:\", episode)\n",
    "    entity1.take_a_walk()\n",
    "    \n",
    "state_action_values1 = entity1.give_state_action_values()\n",
    "\n",
    "policy = np.zeros((dims[0],dims[1],4))\n",
    "\n",
    "for key in state_action_values1:\n",
    "    i =  key[0]\n",
    "    j = key[1]\n",
    "    policy[i,j] = state_action_values1[key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [-21.60926961542814,\n",
       "  -18.26866560606853,\n",
       "  -4.999999943460896,\n",
       "  -18.287555072000007],\n",
       " (0, 1): [-4.976388167585652,\n",
       "  -5.191493849826956,\n",
       "  -7.864966787939983,\n",
       "  -45.53151053406853],\n",
       " (0, 2): [-5.028964776920135,\n",
       "  -37.39420313981574,\n",
       "  -25.742915334068527,\n",
       "  -4.824078139555841],\n",
       " (0, 3): [-10.437630797334064,\n",
       "  -9.354362367851456,\n",
       "  -4.997971759039636,\n",
       "  -55.67288842725483],\n",
       " (0, 4): [-4.999999999999485,\n",
       "  -4.996038591874289,\n",
       "  -4.161139200000001,\n",
       "  -37.410805209482064],\n",
       " (0, 5): [-4.999888496274009,\n",
       "  -4.993810299803575,\n",
       "  -3.9514240000000007,\n",
       "  -4.99999042190287],\n",
       " (0, 6): [-4.999782219285171,\n",
       "  -7.782640861309784,\n",
       "  -3.6892800000000006,\n",
       "  -4.4631290880000005],\n",
       " (0, 7): [-4.987910741803855,\n",
       "  -4.990328593443085,\n",
       "  -3.3616000000000006,\n",
       "  -4.976388167585652],\n",
       " (0, 8): [-4.9704852094820655,\n",
       "  -4.953883139815727,\n",
       "  -4.9704852094820655,\n",
       "  -2.9520000000000004],\n",
       " (0, 9): [-4.887410009315738,\n",
       "  -2.9520000000000004,\n",
       "  -4.780097674444801,\n",
       "  -2.4400000000000004],\n",
       " (1, 0): [-4.997464698799547,\n",
       "  -6.423571235450405,\n",
       "  -4.999468308801687,\n",
       "  -4.998377407231709],\n",
       " (1, 1): [-30.952407717058545,\n",
       "  -5.467514281798084,\n",
       "  -4.656402616320001,\n",
       "  -84.18111053406852],\n",
       " (1, 2): [-55.65110651185259,\n",
       "  -4.9989615406282955,\n",
       "  -4.570503270400001,\n",
       "  -4.999992337522297],\n",
       " (1, 3): [-4.999335386002109,\n",
       "  -6.780890151238264,\n",
       "  -4.656402616320001,\n",
       "  -18.288968047187126],\n",
       " (1, 4): [-37.81542009305042,\n",
       "  -4.328911360000001,\n",
       "  -5.37401141595759,\n",
       "  -4.999999889572059],\n",
       " (1, 5): [-4.999910797019206,\n",
       "  -25.761804800000007,\n",
       "  -18.287443568274014,\n",
       "  -68.2699280074526],\n",
       " (1, 6): [-45.54421029980358,\n",
       "  -4.4631290880000005,\n",
       "  -68.35999999998533,\n",
       "  -84.19797175903965],\n",
       " (1, 7): [-4.976388167585652,\n",
       "  -4.927942405962073,\n",
       "  -2.9520000000000004,\n",
       "  -4.990328593443081],\n",
       " (1, 8): [-4.9704852094820655,\n",
       "  -4.859262511644673,\n",
       "  -2.4400000000000004,\n",
       "  -4.953883139815727],\n",
       " (1, 9): [-4.780097674444801, -4.161139200000001, -4.161139200000001, -1.8],\n",
       " (2, 0): [-55.61594240596209,\n",
       "  -30.951724308801687,\n",
       "  -4.780097674444801,\n",
       "  -10.780806407943942],\n",
       " (2, 1): [-4.999999999996153,\n",
       "  -4.725122093056001,\n",
       "  -4.999999999992486,\n",
       "  -103.99032859344308],\n",
       " (2, 2): [-4.999999999992486,\n",
       "  -5.000000005279054,\n",
       "  -8.47830107663723,\n",
       "  -103.99999871444956],\n",
       " (2, 3): [-4.927942405962073,\n",
       "  -30.95208177542814,\n",
       "  -84.19870192578537,\n",
       "  -120.58583200758567],\n",
       " (2, 4): [-4.859262511644673,\n",
       "  -30.952255998981485,\n",
       "  -25.742915334068527,\n",
       "  -103.99837740723171],\n",
       " (2, 5): [-68.32310651185259,\n",
       "  -4.998701925785367,\n",
       "  -4.997971759039636,\n",
       "  -103.94235392476966],\n",
       " (2, 6): [-84.12794240596207,\n",
       "  -4.997464698799545,\n",
       "  -4.997464698799545,\n",
       "  -103.96310651185259],\n",
       " (2, 7): [-4.997464698799544,\n",
       "  -55.61594240596209,\n",
       "  -84.19999986196508,\n",
       "  -103.90992800745259],\n",
       " (2, 8): [-4.942353924769659, -3.3616000000000006, -1.8, -103.85926251164467],\n",
       " (2, 9): [-2.4400000000000004, -2.9520000000000004, -1.8, -1.0],\n",
       " (3, 0): [-97.48755507199988,\n",
       "  -4.824078139555841,\n",
       "  -103.85926251164467,\n",
       "  -59.168083710293814],\n",
       " (3, 1): [0, 0, 0, 0],\n",
       " (3, 2): [0, 0, 0, 0],\n",
       " (3, 3): [0, 0, 0, 0],\n",
       " (3, 4): [0, 0, 0, 0],\n",
       " (3, 5): [0, 0, 0, 0],\n",
       " (3, 6): [0, 0, 0, 0],\n",
       " (3, 7): [0, 0, 0, 0],\n",
       " (3, 8): [0, 0, 0, 0],\n",
       " (3, 9): [0, 0, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing policy walk\n",
      "current state is: [3, 0]\n",
      "current step is: 0\n",
      "next action is: up\n",
      "current state is: [2, 0]\n",
      "current step is: 1\n",
      "next action is: right\n",
      "current state is: [2, 1]\n",
      "current step is: 2\n",
      "next action is: up\n",
      "current state is: [1, 1]\n",
      "current step is: 3\n",
      "next action is: right\n",
      "current state is: [1, 2]\n",
      "current step is: 4\n",
      "next action is: right\n",
      "current state is: [1, 3]\n",
      "current step is: 5\n",
      "next action is: right\n",
      "current state is: [1, 4]\n",
      "current step is: 6\n",
      "next action is: up\n",
      "current state is: [0, 4]\n",
      "current step is: 7\n",
      "next action is: right\n",
      "current state is: [0, 5]\n",
      "current step is: 8\n",
      "next action is: right\n",
      "current state is: [0, 6]\n",
      "current step is: 9\n",
      "next action is: right\n",
      "current state is: [0, 7]\n",
      "current step is: 10\n",
      "next action is: right\n",
      "current state is: [0, 8]\n",
      "current step is: 11\n",
      "next action is: down\n",
      "current state is: [1, 8]\n",
      "current step is: 12\n",
      "next action is: right\n",
      "current state is: [1, 9]\n",
      "current step is: 13\n",
      "next action is: down\n",
      "current state is: [2, 9]\n",
      "current step is: 14\n",
      "next action is: down\n",
      "current state is: [3, 9]\n",
      "current step is: 15\n",
      "next action is: left\n",
      "you have reached the terminal state in  16  steps!\n"
     ]
    }
   ],
   "source": [
    "#optimal steps for this problem is 15, book notes 17 is the average on their solution\n",
    "#this method sets epsilon to 0 and uses the q values to do a pure exploitation.\n",
    "print(\"doing policy walk\")\n",
    "entity1.policy_walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arbitrary state_action_values\n",
    "#using a dict because look up is much easier then\n",
    "state_action_values2 = {(None,None,None):0}\n",
    "\n",
    "for i in range(dims[0]):\n",
    "    for j in range(dims[1]):\n",
    "        state_action_values2[(i,j)] = [0,0,0,0]\n",
    "        \n",
    "state_action_values2.pop((None,None,None))\n",
    "            \n",
    "#state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment2 = environment()\n",
    "entity2 = q_entity(state_action_values2, environment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_episodes = 1000\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    #if episode%100 == 0:\n",
    "        #print(\"episode #:\", episode)\n",
    "    entity2.take_a_walk()\n",
    "    \n",
    "state_action_values2 = entity2.give_state_action_values()\n",
    "\n",
    "policy = np.zeros((dims[0],dims[1],4))\n",
    "\n",
    "for key in state_action_values2:\n",
    "    i =  key[0]\n",
    "    j = key[1]\n",
    "    policy[i,j] = state_action_values2[key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [-1.9982408443666682,\n",
       "  -1.9982804385274122,\n",
       "  -1.9981193548532306,\n",
       "  -1.9983997310623263],\n",
       " (0, 1): [-1.997479450665053,\n",
       "  -1.9974524612537783,\n",
       "  -1.9972541496998262,\n",
       "  -1.9975737085723215],\n",
       " (0, 2): [-1.996553971956633,\n",
       "  -1.9957006734299683,\n",
       "  -1.995468221859371,\n",
       "  -1.9954689174980909],\n",
       " (0, 3): [-1.9932354080587757,\n",
       "  -1.9925916617531823,\n",
       "  -1.992029356991292,\n",
       "  -1.992061136519233],\n",
       " (0, 4): [-1.987545694011522,\n",
       "  -1.9871289304854725,\n",
       "  -1.9863323252991836,\n",
       "  -1.987100548057465],\n",
       " (0, 5): [-1.979171725202673,\n",
       "  -1.9751982396639427,\n",
       "  -1.9751914003202544,\n",
       "  -1.9764574717759242],\n",
       " (0, 6): [-1.9613716709883884,\n",
       "  -1.958383693508491,\n",
       "  -1.9543592586721639,\n",
       "  -1.9576328442210083],\n",
       " (0, 7): [-1.9216218707970107,\n",
       "  -1.9188621924725555,\n",
       "  -1.919354347111249,\n",
       "  -1.9187669275432864],\n",
       " (0, 8): [-1.8650181452013181,\n",
       "  -1.8748903681667772,\n",
       "  -1.8555281329901547,\n",
       "  -1.8583783868777959],\n",
       " (0, 9): [-1.7598674067967188,\n",
       "  -1.7602389829542062,\n",
       "  -1.7575419998162065,\n",
       "  -1.742783617302872],\n",
       " (1, 0): [-1.9988438837227573,\n",
       "  -1.998745380427626,\n",
       "  -1.998747250108184,\n",
       "  -1.9988099504310026],\n",
       " (1, 1): [-1.9979638423004302,\n",
       "  -1.9978532293562123,\n",
       "  -1.997760681279928,\n",
       "  -1.9977683000591635],\n",
       " (1, 2): [-1.9960179898032493,\n",
       "  -1.996214809075631,\n",
       "  -1.995781807072301,\n",
       "  -1.9958439244793285],\n",
       " (1, 3): [-1.9940937279625235,\n",
       "  -1.992005507807171,\n",
       "  -1.9919206921961314,\n",
       "  -1.9919500829920342],\n",
       " (1, 4): [-1.9897892549439398,\n",
       "  -1.9876662745133054,\n",
       "  -1.9841729342145058,\n",
       "  -1.9841700897088546],\n",
       " (1, 5): [-1.9717597320979494,\n",
       "  -1.9744376494784068,\n",
       "  -1.9686241700710558,\n",
       "  -1.9686137450083139],\n",
       " (1, 6): [-1.954847340457938,\n",
       "  -1.9387160090068833,\n",
       "  -1.9374383153257622,\n",
       "  -1.9374243006318577],\n",
       " (1, 7): [-1.8913161356812016,\n",
       "  -1.9011530333018642,\n",
       "  -1.874974280118497,\n",
       "  -1.8749739430006702],\n",
       " (1, 8): [-1.788194019209667,\n",
       "  -1.7652854265186362,\n",
       "  -1.7499960576703641,\n",
       "  -1.7499955341155162],\n",
       " (1, 9): [-1.7641239693849946,\n",
       "  -1.6272678529216174,\n",
       "  -1.5896319009124396,\n",
       "  -1.4999999949421814],\n",
       " (2, 0): [-1.9990172988923218,\n",
       "  -1.9993122325611832,\n",
       "  -1.9980468749999996,\n",
       "  -1.999484455301936],\n",
       " (2, 1): [-1.9990145412715856,\n",
       "  -1.998694303073076,\n",
       "  -1.9960937499999996,\n",
       "  -100.96734979604332],\n",
       " (2, 2): [-1.997943128058685,\n",
       "  -1.9973780037466393,\n",
       "  -1.9921874999999996,\n",
       "  -100.57149607811962],\n",
       " (2, 3): [-1.9959964200596596,\n",
       "  -1.9949682701909188,\n",
       "  -1.9843749999999996,\n",
       "  -99.98448666336594],\n",
       " (2, 4): [-1.9913368381538166,\n",
       "  -1.9918039532085619,\n",
       "  -1.9687499999999996,\n",
       "  -100.23883748340049],\n",
       " (2, 5): [-1.9839618334306355,\n",
       "  -1.983954692641015,\n",
       "  -1.9374999999999996,\n",
       "  -100.86423037084505],\n",
       " (2, 6): [-1.9686724686852388,\n",
       "  -1.9672251783967045,\n",
       "  -1.8749999999999996,\n",
       "  -100.23939991891622],\n",
       " (2, 7): [-1.9353280957410046,\n",
       "  -1.936353781250639,\n",
       "  -1.7499999999999996,\n",
       "  -99.64857441421726],\n",
       " (2, 8): [-1.8652906815730972,\n",
       "  -1.8745236756304502,\n",
       "  -1.4999999999999996,\n",
       "  -100.23971749617799],\n",
       " (2, 9): [-1.7447100389173849,\n",
       "  -1.734544497876278,\n",
       "  -1.4832077053524297,\n",
       "  -0.9999999999999998],\n",
       " (3, 0): [-1.999510898098087,\n",
       "  -1.9990234374999996,\n",
       "  -99.98386179869138,\n",
       "  -1.9995087022260782],\n",
       " (3, 1): [0, 0, 0, 0],\n",
       " (3, 2): [0, 0, 0, 0],\n",
       " (3, 3): [0, 0, 0, 0],\n",
       " (3, 4): [0, 0, 0, 0],\n",
       " (3, 5): [0, 0, 0, 0],\n",
       " (3, 6): [0, 0, 0, 0],\n",
       " (3, 7): [0, 0, 0, 0],\n",
       " (3, 8): [0, 0, 0, 0],\n",
       " (3, 9): [0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing policy walk\n",
      "current state is: [3, 0]\n",
      "current step is: 0\n",
      "next action is: up\n",
      "current state is: [2, 0]\n",
      "current step is: 1\n",
      "next action is: right\n",
      "current state is: [2, 1]\n",
      "current step is: 2\n",
      "next action is: right\n",
      "current state is: [2, 2]\n",
      "current step is: 3\n",
      "next action is: right\n",
      "current state is: [2, 3]\n",
      "current step is: 4\n",
      "next action is: right\n",
      "current state is: [2, 4]\n",
      "current step is: 5\n",
      "next action is: right\n",
      "current state is: [2, 5]\n",
      "current step is: 6\n",
      "next action is: right\n",
      "current state is: [2, 6]\n",
      "current step is: 7\n",
      "next action is: right\n",
      "current state is: [2, 7]\n",
      "current step is: 8\n",
      "next action is: right\n",
      "current state is: [2, 8]\n",
      "current step is: 9\n",
      "next action is: right\n",
      "current state is: [2, 9]\n",
      "current step is: 10\n",
      "next action is: down\n",
      "current state is: [3, 9]\n",
      "current step is: 11\n",
      "next action is: left\n",
      "you have reached the terminal state in  12  steps!\n"
     ]
    }
   ],
   "source": [
    "#optimal steps for this problem is 15, book notes 17 is the average on their solution\n",
    "#this method sets epsilon to 0 and uses the q values to do a pure exploitation.\n",
    "print(\"doing policy walk\")\n",
    "entity2.policy_walk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "SARSA takes 16 steps because it goes to the upper edge of the grid to avoid the cliff while q takes 12 steps because it goes and walks along the cliff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
