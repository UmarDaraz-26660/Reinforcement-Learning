{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking Q-Learning v SARSA \n",
    "SARSA on-policy control v q-learning off-policy control\n",
    "\n",
    "Comparison of methods as per the example in the Sutton Barto Text.\n",
    "\n",
    "Objects to create:\n",
    "- environment to simulate the 'cliff walk'\n",
    "- entity to simulate the traversal thru the paths\n",
    "    \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "The blue line represents the expected SARSA path and the red is the expected Q-learning path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment object\n",
    "class environment():\n",
    "    def __init__(self, dims = [4,10], start = [3,0] , terminal = [3,9], reward = -1, the_cliff_reward = -100, the_cliff = [(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(3,7),(3,8)]):\n",
    "        \"\"\"\n",
    "        this class initializes the environment\n",
    "        \n",
    "        inputs\n",
    "        dims - a scalar to define a dxd grid environment\n",
    "        terminals - a list of lists of ordered pairs representing the location in the grid of the terminal states\n",
    "        reward -  the reward value for moving one space (unless in a terminal state)\n",
    "        \n",
    "        self defined variables\n",
    "        actions - determines actions that can be taken in the environment where 0 = left, 1 = up, 2 = right, 3 = down\n",
    "        \n",
    "        returns\n",
    "        None\n",
    "        \n",
    "        cheating here as i did not do checks on things for integrity...\n",
    "        \"\"\"\n",
    "        self.dims = dims\n",
    "        self.start = start\n",
    "        self.terminal = terminal\n",
    "        self.reward = reward\n",
    "        self.the_cliff_reward = the_cliff_reward\n",
    "        self.the_cliff = the_cliff\n",
    "        self.actions = [np.array([0, -1]),\n",
    "                       np.array([-1, 0]),\n",
    "                       np.array([0, 1]),\n",
    "                       np.array([1, 0])]\n",
    "            \n",
    "    def is_terminal(self,i,j):\n",
    "        \"\"\"\n",
    "        This function checks if coordinates are for a terminal state\n",
    "        \n",
    "        inputs\n",
    "        i = y coord (row)\n",
    "        j = x coord (col)\n",
    "        \n",
    "        returns\n",
    "        boolean on terminal status\n",
    "        \"\"\"\n",
    "        #print(\"episode end\")\n",
    "        return [i,j] == self.terminal\n",
    "        \n",
    "        \n",
    "    def next_state(self,i,j,a):\n",
    "        \"\"\"\n",
    "        this function determines the next state and the reward received\n",
    "        This function also checks if current state is terminal, or next state is terminal\n",
    "        This function will also bounce back to the original coordinates if it is an edge/border square in the grid\n",
    "        \n",
    "        inputs\n",
    "        i = y coord (row)\n",
    "        j = x coord (col)\n",
    "        a = action taken where 0 = left, 1 = up, 2 = right, 3 = down\n",
    "        \n",
    "        returns\n",
    "        i_next, j_next representing next state coordinates\n",
    "        reward received for moving\n",
    "        \"\"\"\n",
    "        # check if current state is terminal, if so return current state and reward = 0\n",
    "        #i = row, j = col\n",
    "        if self.is_terminal(i,j):\n",
    "            return i, j, 0\n",
    "        \n",
    "        # get next state\n",
    "        next = [i,j] + self.actions[a] \n",
    "        #print(next)\n",
    "        \n",
    "        #check if move takes off grid on top or left go back to original space\n",
    "        if next[0] < 0 or next[1] < 0:\n",
    "            return i, j, self.reward\n",
    "        #check if next is the cliff go back to start\n",
    "        elif (next[0], next[1]) in self.the_cliff:\n",
    "            return self.start[0], self.start[1], self.the_cliff_reward\n",
    "        #check if off bottom, or right go back to original space\n",
    "        elif (next[0] > (self.dims[0]-1)) or (next[1] > (self.dims[1]-1)):\n",
    "            return i, j, self.reward \n",
    "        else:\n",
    "            return next[0], next[1], self.reward \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sarsa_entity():\n",
    "    def __init__(self, state_action_values, environment, algo_param_alpha = 1, gamma = 0.8, epsilon = 0.05, start = [3,0]):\n",
    "        \"\"\"\n",
    "        This class initializes the entity which will \n",
    "        walk thru the grid world and collect data\n",
    "        this follows a puerly greedy policy atm\n",
    "        \n",
    "        inputs\n",
    "        rand_policy = policy ot follow if not initialized\n",
    "        state_action_values - calculated state-action values for all states except the terminal state\n",
    "        algo_param_alpha - step size to be used\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_action_values = state_action_values\n",
    "        self.algo_param_alpha = algo_param_alpha\n",
    "        self.gamma = gamma\n",
    "        self.environment = environment\n",
    "        self.start_location = start\n",
    "        self.current_location = start\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "                \n",
    "    def pick_action(self, state_action_values):\n",
    "        #need to fix this asap\n",
    "        \n",
    "        #simple pick action based in random policy to go any direction with equal probability\n",
    "        greedy_action = np.argmax(state_action_values)\n",
    "        #print(greedy_action)\n",
    "        \n",
    "        action = greedy_action\n",
    "        \n",
    "        if self.epsilon == 0:\n",
    "            return action\n",
    "        \n",
    "        if np.random.binomial(1, self.epsilon, 1):\n",
    "            temp_list = [0,1,2,3]\n",
    "            #print(temp_list)\n",
    "            temp_list.pop(greedy_action)\n",
    "            #print(temp_list)\n",
    "            action = np.random.choice(temp_list)\n",
    "            #print(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        next_i, next_j, reward = self.environment.next_state(self.current_location[0], self.current_location[1], action)\n",
    "        return next_i, next_j, reward\n",
    "    \n",
    "    def restart_walk(self):\n",
    "        self.current_location = self.start_location\n",
    "    \n",
    "    def take_a_walk(self):\n",
    "        \n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "        \n",
    "        #choose first action according to e-greedy\n",
    "        i, j = self.current_location[0], self.current_location[1]\n",
    "        state_action_values = self.state_action_values[(i,j)]\n",
    "        action = self.pick_action(state_action_values)\n",
    "        next_i, next_j, reward = self.take_action(action)\n",
    "        \n",
    "        while not terminal:\n",
    "\n",
    "            state_action_values = self.state_action_values[(next_i, next_j)]\n",
    "            next_action = self.pick_action(state_action_values)\n",
    "            next_action_value = state_action_values[next_action]\n",
    "            \n",
    "            #update last state value\n",
    "            self.state_action_values[(i,j)][action] = \\\n",
    "            (self.state_action_values[(i,j)][action] + \\\n",
    "            (self.algo_param_alpha * \\\n",
    "            (reward + \\\n",
    "            (self.gamma * next_action_value) - \\\n",
    "            self.state_action_values[(i,j)][action])))\n",
    "             \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j] \n",
    "            i, j = next_i, next_j\n",
    "            \n",
    "            #update to iterate \n",
    "            action = next_action\n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "            \n",
    "            \n",
    "            if reward == 0:\n",
    "                terminal = True\n",
    "                \n",
    "    def policy_walk(self):\n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "        \n",
    "        action_dict = {0 : \"left\", 1 : \"up\", 2 : \"right\", 3 : \"down\"}\n",
    "        \n",
    "        number_of_steps = 0\n",
    "        \n",
    "        orig_epsilon = self.epsilon\n",
    "        \n",
    "        self.epsilon = 0\n",
    "        \n",
    "        while not terminal:\n",
    "            print(\"current state is:\", self.current_location)\n",
    "            print(\"current step is:\", number_of_steps)\n",
    "            state_action_values = self.state_action_values[(self.current_location[0],self.current_location[1])]\n",
    "            \n",
    "            action = self.pick_action(state_action_values)\n",
    "            print(\"next action is:\", action_dict[action])\n",
    "            \n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "            \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j] \n",
    "            \n",
    "            number_of_steps = number_of_steps + 1\n",
    "            \n",
    "            if number_of_steps > 100:\n",
    "                print(\"walk failure\")\n",
    "                reward = 0\n",
    "            \n",
    "            if reward == 0:\n",
    "                print(\"you have reached the terminal state in \", number_of_steps, \" steps!\")\n",
    "                self.epsilon = orig_epsilon                      \n",
    "                terminal = True          \n",
    "                \n",
    "    def give_state_action_values(self):\n",
    "        return self.state_action_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_entity():\n",
    "    def __init__(self, state_action_values, environment, algo_param_alpha = 0.25, gamma = 0.5, epsilon = 0.05, start = [3,0]):\n",
    "        \"\"\"\n",
    "        This class initializes the entity which will \n",
    "        walk thru the grid world and collect data\n",
    "        this follows a puerly greedy policy atm\n",
    "        \n",
    "        inputs\n",
    "        rand_policy = policy ot follow if not initialized\n",
    "        state_action_values - calculated state-action values for all states except the terminal state\n",
    "        algo_param_alpha - step size to be used\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_action_values = state_action_values\n",
    "        self.algo_param_alpha = algo_param_alpha\n",
    "        self.gamma = gamma\n",
    "        self.environment = environment\n",
    "        self.start_location = start\n",
    "        self.current_location = start\n",
    "        self.epsilon = epsilon\n",
    "        self.orig_epslion = epsilon\n",
    "\n",
    "                \n",
    "    def pick_action(self, state_action_values):\n",
    "        #need to fix this asap\n",
    "        \n",
    "        #simple pick action based in random policy to go any direction with equal probability\n",
    "        greedy_action = np.argmax(state_action_values)\n",
    "        #print(greedy_action)\n",
    "        \n",
    "        action = greedy_action\n",
    "        \n",
    "        if self.epsilon == 0:\n",
    "            return action\n",
    "        \n",
    "        if np.random.binomial(1, self.epsilon, 1):\n",
    "            temp_list = [0,1,2,3]\n",
    "            #print(temp_list)\n",
    "            temp_list.pop(greedy_action)\n",
    "            #print(temp_list)\n",
    "            action = np.random.choice(temp_list)\n",
    "            #print(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        next_i, next_j, reward = self.environment.next_state(self.current_location[0], self.current_location[1], action)\n",
    "        return next_i, next_j, reward\n",
    "    \n",
    "    def restart_walk(self):\n",
    "        self.current_location = self.start_location\n",
    "    \n",
    "    def take_a_walk(self):\n",
    "        \n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "               \n",
    "        while not terminal:\n",
    "            #choose actual action being taken according to e-greedy\n",
    "            i, j = self.current_location[0], self.current_location[1]\n",
    "            state_action_values = self.state_action_values[(i,j)]\n",
    "            action = self.pick_action(state_action_values)\n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "\n",
    "            state_action_values = self.state_action_values[(next_i, next_j)]\n",
    "            \n",
    "            #flip epsilon to 0 and pick next action as a greedy choice\n",
    "            self.epsilon = 0\n",
    "            next_action = self.pick_action(state_action_values)\n",
    "            self.epsilon = self.orig_epslion\n",
    "            \n",
    "            next_action_value = state_action_values[next_action]\n",
    "            \n",
    "            #update last state value\n",
    "            self.state_action_values[(i,j)][action] = \\\n",
    "            (self.state_action_values[(i,j)][action] + \\\n",
    "            (self.algo_param_alpha * \\\n",
    "            (reward + \\\n",
    "            (self.gamma * next_action_value) - \\\n",
    "            self.state_action_values[(i,j)][action])))\n",
    "             \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j]          \n",
    "            \n",
    "            if reward == 0:\n",
    "                terminal = True\n",
    "                \n",
    "    def policy_walk(self):\n",
    "        self.restart_walk()\n",
    "        \n",
    "        terminal = False\n",
    "        \n",
    "        action_dict = {0 : \"left\", 1 : \"up\", 2 : \"right\", 3 : \"down\"}\n",
    "        \n",
    "        number_of_steps = 0\n",
    "        \n",
    "        orig_epsilon = self.epsilon\n",
    "        \n",
    "        self.epsilon = 0\n",
    "        \n",
    "        while not terminal:\n",
    "            print(\"current state is:\", self.current_location)\n",
    "            print(\"current step is:\", number_of_steps)\n",
    "            state_action_values = self.state_action_values[(self.current_location[0],self.current_location[1])]\n",
    "            \n",
    "            action = self.pick_action(state_action_values)\n",
    "            print(\"next action is:\", action_dict[action])\n",
    "            \n",
    "            next_i, next_j, reward = self.take_action(action)\n",
    "            \n",
    "            #update location\n",
    "            self.current_location = [next_i, next_j] \n",
    "            \n",
    "            number_of_steps = number_of_steps + 1\n",
    "            \n",
    "            if number_of_steps > 100:\n",
    "                print(\"walk failure\")\n",
    "                reward = 0\n",
    "            \n",
    "            if reward == 0:\n",
    "                print(\"you have reached the terminal state in \", number_of_steps, \" steps!\")\n",
    "                self.epsilon = orig_epsilon                      \n",
    "                terminal = True          \n",
    "                \n",
    "    def give_state_action_values(self):\n",
    "        return self.state_action_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step size\n",
    "algo_param_alpha = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions\n",
    "dims = [4,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arbitrary state_action_values\n",
    "#using a dict because look up is much easier then\n",
    "state_action_values1 = {(None,None,None):0}\n",
    "\n",
    "for i in range(dims[0]):\n",
    "    for j in range(dims[1]):\n",
    "        state_action_values1[(i,j)] = [0,0,0,0]\n",
    "        \n",
    "state_action_values1.pop((None,None,None))\n",
    "            \n",
    "#state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment1 = environment()\n",
    "entity1 = sarsa_entity(state_action_values1, environment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #: 0\n",
      "episode #: 1\n",
      "episode #: 2\n",
      "episode #: 3\n",
      "episode #: 4\n",
      "episode #: 5\n",
      "episode #: 6\n",
      "episode #: 7\n",
      "episode #: 8\n",
      "episode #: 9\n",
      "episode #: 10\n",
      "episode #: 11\n",
      "episode #: 12\n",
      "episode #: 13\n",
      "episode #: 14\n",
      "episode #: 15\n",
      "episode #: 16\n",
      "episode #: 17\n",
      "episode #: 18\n",
      "episode #: 19\n",
      "episode #: 20\n",
      "episode #: 21\n",
      "episode #: 22\n",
      "episode #: 23\n",
      "episode #: 24\n",
      "episode #: 25\n",
      "episode #: 26\n",
      "episode #: 27\n",
      "episode #: 28\n",
      "episode #: 29\n",
      "episode #: 30\n",
      "episode #: 31\n",
      "episode #: 32\n",
      "episode #: 33\n",
      "episode #: 34\n",
      "episode #: 35\n",
      "episode #: 36\n",
      "episode #: 37\n",
      "episode #: 38\n",
      "episode #: 39\n",
      "episode #: 40\n",
      "episode #: 41\n",
      "episode #: 42\n",
      "episode #: 43\n",
      "episode #: 44\n",
      "episode #: 45\n",
      "episode #: 46\n",
      "episode #: 47\n",
      "episode #: 48\n",
      "episode #: 49\n",
      "episode #: 50\n",
      "episode #: 51\n",
      "episode #: 52\n",
      "episode #: 53\n",
      "episode #: 54\n",
      "episode #: 55\n",
      "episode #: 56\n",
      "episode #: 57\n",
      "episode #: 58\n",
      "episode #: 59\n",
      "episode #: 60\n",
      "episode #: 61\n",
      "episode #: 62\n",
      "episode #: 63\n",
      "episode #: 64\n",
      "episode #: 65\n",
      "episode #: 66\n",
      "episode #: 67\n",
      "episode #: 68\n",
      "episode #: 69\n",
      "episode #: 70\n",
      "episode #: 71\n",
      "episode #: 72\n",
      "episode #: 73\n",
      "episode #: 74\n",
      "episode #: 75\n",
      "episode #: 76\n",
      "episode #: 77\n",
      "episode #: 78\n",
      "episode #: 79\n",
      "episode #: 80\n",
      "episode #: 81\n",
      "episode #: 82\n",
      "episode #: 83\n",
      "episode #: 84\n",
      "episode #: 85\n",
      "episode #: 86\n",
      "episode #: 87\n",
      "episode #: 88\n",
      "episode #: 89\n",
      "episode #: 90\n",
      "episode #: 91\n",
      "episode #: 92\n",
      "episode #: 93\n",
      "episode #: 94\n",
      "episode #: 95\n",
      "episode #: 96\n",
      "episode #: 97\n",
      "episode #: 98\n",
      "episode #: 99\n",
      "episode #: 100\n",
      "episode #: 101\n",
      "episode #: 102\n",
      "episode #: 103\n",
      "episode #: 104\n",
      "episode #: 105\n",
      "episode #: 106\n",
      "episode #: 107\n",
      "episode #: 108\n",
      "episode #: 109\n",
      "episode #: 110\n",
      "episode #: 111\n",
      "episode #: 112\n",
      "episode #: 113\n",
      "episode #: 114\n",
      "episode #: 115\n",
      "episode #: 116\n",
      "episode #: 117\n",
      "episode #: 118\n",
      "episode #: 119\n",
      "episode #: 120\n",
      "episode #: 121\n",
      "episode #: 122\n",
      "episode #: 123\n",
      "episode #: 124\n",
      "episode #: 125\n",
      "episode #: 126\n",
      "episode #: 127\n",
      "episode #: 128\n",
      "episode #: 129\n",
      "episode #: 130\n",
      "episode #: 131\n",
      "episode #: 132\n",
      "episode #: 133\n",
      "episode #: 134\n",
      "episode #: 135\n",
      "episode #: 136\n",
      "episode #: 137\n",
      "episode #: 138\n",
      "episode #: 139\n",
      "episode #: 140\n",
      "episode #: 141\n",
      "episode #: 142\n",
      "episode #: 143\n",
      "episode #: 144\n",
      "episode #: 145\n",
      "episode #: 146\n",
      "episode #: 147\n",
      "episode #: 148\n",
      "episode #: 149\n",
      "episode #: 150\n",
      "episode #: 151\n",
      "episode #: 152\n",
      "episode #: 153\n",
      "episode #: 154\n",
      "episode #: 155\n",
      "episode #: 156\n",
      "episode #: 157\n",
      "episode #: 158\n",
      "episode #: 159\n",
      "episode #: 160\n",
      "episode #: 161\n",
      "episode #: 162\n",
      "episode #: 163\n",
      "episode #: 164\n",
      "episode #: 165\n",
      "episode #: 166\n",
      "episode #: 167\n",
      "episode #: 168\n",
      "episode #: 169\n",
      "episode #: 170\n",
      "episode #: 171\n",
      "episode #: 172\n",
      "episode #: 173\n",
      "episode #: 174\n",
      "episode #: 175\n",
      "episode #: 176\n",
      "episode #: 177\n",
      "episode #: 178\n",
      "episode #: 179\n",
      "episode #: 180\n",
      "episode #: 181\n",
      "episode #: 182\n",
      "episode #: 183\n",
      "episode #: 184\n",
      "episode #: 185\n",
      "episode #: 186\n",
      "episode #: 187\n",
      "episode #: 188\n",
      "episode #: 189\n",
      "episode #: 190\n",
      "episode #: 191\n",
      "episode #: 192\n",
      "episode #: 193\n",
      "episode #: 194\n",
      "episode #: 195\n",
      "episode #: 196\n",
      "episode #: 197\n",
      "episode #: 198\n",
      "episode #: 199\n",
      "episode #: 200\n",
      "episode #: 201\n",
      "episode #: 202\n",
      "episode #: 203\n",
      "episode #: 204\n",
      "episode #: 205\n",
      "episode #: 206\n",
      "episode #: 207\n",
      "episode #: 208\n",
      "episode #: 209\n",
      "episode #: 210\n",
      "episode #: 211\n",
      "episode #: 212\n",
      "episode #: 213\n",
      "episode #: 214\n",
      "episode #: 215\n",
      "episode #: 216\n",
      "episode #: 217\n",
      "episode #: 218\n",
      "episode #: 219\n",
      "episode #: 220\n",
      "episode #: 221\n",
      "episode #: 222\n",
      "episode #: 223\n",
      "episode #: 224\n",
      "episode #: 225\n",
      "episode #: 226\n",
      "episode #: 227\n",
      "episode #: 228\n",
      "episode #: 229\n",
      "episode #: 230\n",
      "episode #: 231\n",
      "episode #: 232\n",
      "episode #: 233\n",
      "episode #: 234\n",
      "episode #: 235\n",
      "episode #: 236\n",
      "episode #: 237\n",
      "episode #: 238\n",
      "episode #: 239\n",
      "episode #: 240\n",
      "episode #: 241\n",
      "episode #: 242\n",
      "episode #: 243\n",
      "episode #: 244\n",
      "episode #: 245\n",
      "episode #: 246\n",
      "episode #: 247\n",
      "episode #: 248\n",
      "episode #: 249\n",
      "episode #: 250\n",
      "episode #: 251\n",
      "episode #: 252\n",
      "episode #: 253\n",
      "episode #: 254\n",
      "episode #: 255\n",
      "episode #: 256\n",
      "episode #: 257\n",
      "episode #: 258\n",
      "episode #: 259\n",
      "episode #: 260\n",
      "episode #: 261\n",
      "episode #: 262\n",
      "episode #: 263\n",
      "episode #: 264\n",
      "episode #: 265\n",
      "episode #: 266\n",
      "episode #: 267\n",
      "episode #: 268\n",
      "episode #: 269\n",
      "episode #: 270\n",
      "episode #: 271\n",
      "episode #: 272\n",
      "episode #: 273\n",
      "episode #: 274\n",
      "episode #: 275\n",
      "episode #: 276\n",
      "episode #: 277\n",
      "episode #: 278\n",
      "episode #: 279\n",
      "episode #: 280\n",
      "episode #: 281\n",
      "episode #: 282\n",
      "episode #: 283\n",
      "episode #: 284\n",
      "episode #: 285\n",
      "episode #: 286\n",
      "episode #: 287\n",
      "episode #: 288\n",
      "episode #: 289\n",
      "episode #: 290\n",
      "episode #: 291\n",
      "episode #: 292\n",
      "episode #: 293\n",
      "episode #: 294\n",
      "episode #: 295\n",
      "episode #: 296\n",
      "episode #: 297\n",
      "episode #: 298\n",
      "episode #: 299\n",
      "episode #: 300\n",
      "episode #: 301\n",
      "episode #: 302\n",
      "episode #: 303\n",
      "episode #: 304\n",
      "episode #: 305\n",
      "episode #: 306\n",
      "episode #: 307\n",
      "episode #: 308\n",
      "episode #: 309\n",
      "episode #: 310\n",
      "episode #: 311\n",
      "episode #: 312\n",
      "episode #: 313\n",
      "episode #: 314\n",
      "episode #: 315\n",
      "episode #: 316\n",
      "episode #: 317\n",
      "episode #: 318\n",
      "episode #: 319\n",
      "episode #: 320\n",
      "episode #: 321\n",
      "episode #: 322\n",
      "episode #: 323\n",
      "episode #: 324\n",
      "episode #: 325\n",
      "episode #: 326\n",
      "episode #: 327\n",
      "episode #: 328\n",
      "episode #: 329\n",
      "episode #: 330\n",
      "episode #: 331\n",
      "episode #: 332\n",
      "episode #: 333\n",
      "episode #: 334\n",
      "episode #: 335\n",
      "episode #: 336\n",
      "episode #: 337\n",
      "episode #: 338\n",
      "episode #: 339\n",
      "episode #: 340\n",
      "episode #: 341\n",
      "episode #: 342\n",
      "episode #: 343\n",
      "episode #: 344\n",
      "episode #: 345\n",
      "episode #: 346\n",
      "episode #: 347\n",
      "episode #: 348\n",
      "episode #: 349\n",
      "episode #: 350\n",
      "episode #: 351\n",
      "episode #: 352\n",
      "episode #: 353\n",
      "episode #: 354\n",
      "episode #: 355\n",
      "episode #: 356\n",
      "episode #: 357\n",
      "episode #: 358\n",
      "episode #: 359\n",
      "episode #: 360\n",
      "episode #: 361\n",
      "episode #: 362\n",
      "episode #: 363\n",
      "episode #: 364\n",
      "episode #: 365\n",
      "episode #: 366\n",
      "episode #: 367\n",
      "episode #: 368\n",
      "episode #: 369\n",
      "episode #: 370\n",
      "episode #: 371\n",
      "episode #: 372\n",
      "episode #: 373\n",
      "episode #: 374\n",
      "episode #: 375\n",
      "episode #: 376\n",
      "episode #: 377\n",
      "episode #: 378\n",
      "episode #: 379\n",
      "episode #: 380\n",
      "episode #: 381\n",
      "episode #: 382\n",
      "episode #: 383\n",
      "episode #: 384\n",
      "episode #: 385\n",
      "episode #: 386\n",
      "episode #: 387\n",
      "episode #: 388\n",
      "episode #: 389\n",
      "episode #: 390\n",
      "episode #: 391\n",
      "episode #: 392\n",
      "episode #: 393\n",
      "episode #: 394\n",
      "episode #: 395\n",
      "episode #: 396\n",
      "episode #: 397\n",
      "episode #: 398\n",
      "episode #: 399\n",
      "episode #: 400\n",
      "episode #: 401\n",
      "episode #: 402\n",
      "episode #: 403\n",
      "episode #: 404\n",
      "episode #: 405\n",
      "episode #: 406\n",
      "episode #: 407\n",
      "episode #: 408\n",
      "episode #: 409\n",
      "episode #: 410\n",
      "episode #: 411\n",
      "episode #: 412\n",
      "episode #: 413\n",
      "episode #: 414\n",
      "episode #: 415\n",
      "episode #: 416\n",
      "episode #: 417\n",
      "episode #: 418\n",
      "episode #: 419\n",
      "episode #: 420\n",
      "episode #: 421\n",
      "episode #: 422\n",
      "episode #: 423\n",
      "episode #: 424\n",
      "episode #: 425\n",
      "episode #: 426\n",
      "episode #: 427\n",
      "episode #: 428\n",
      "episode #: 429\n",
      "episode #: 430\n",
      "episode #: 431\n",
      "episode #: 432\n",
      "episode #: 433\n",
      "episode #: 434\n",
      "episode #: 435\n",
      "episode #: 436\n",
      "episode #: 437\n",
      "episode #: 438\n",
      "episode #: 439\n",
      "episode #: 440\n",
      "episode #: 441\n",
      "episode #: 442\n",
      "episode #: 443\n",
      "episode #: 444\n",
      "episode #: 445\n",
      "episode #: 446\n",
      "episode #: 447\n",
      "episode #: 448\n",
      "episode #: 449\n",
      "episode #: 450\n",
      "episode #: 451\n",
      "episode #: 452\n",
      "episode #: 453\n",
      "episode #: 454\n",
      "episode #: 455\n",
      "episode #: 456\n",
      "episode #: 457\n",
      "episode #: 458\n",
      "episode #: 459\n",
      "episode #: 460\n",
      "episode #: 461\n",
      "episode #: 462\n",
      "episode #: 463\n",
      "episode #: 464\n",
      "episode #: 465\n",
      "episode #: 466\n",
      "episode #: 467\n",
      "episode #: 468\n",
      "episode #: 469\n",
      "episode #: 470\n",
      "episode #: 471\n",
      "episode #: 472\n",
      "episode #: 473\n",
      "episode #: 474\n",
      "episode #: 475\n",
      "episode #: 476\n",
      "episode #: 477\n",
      "episode #: 478\n",
      "episode #: 479\n",
      "episode #: 480\n",
      "episode #: 481\n",
      "episode #: 482\n",
      "episode #: 483\n",
      "episode #: 484\n",
      "episode #: 485\n",
      "episode #: 486\n",
      "episode #: 487\n",
      "episode #: 488\n",
      "episode #: 489\n",
      "episode #: 490\n",
      "episode #: 491\n",
      "episode #: 492\n",
      "episode #: 493\n",
      "episode #: 494\n",
      "episode #: 495\n",
      "episode #: 496\n",
      "episode #: 497\n",
      "episode #: 498\n",
      "episode #: 499\n",
      "episode #: 500\n",
      "episode #: 501\n",
      "episode #: 502\n",
      "episode #: 503\n",
      "episode #: 504\n",
      "episode #: 505\n",
      "episode #: 506\n",
      "episode #: 507\n",
      "episode #: 508\n",
      "episode #: 509\n",
      "episode #: 510\n",
      "episode #: 511\n",
      "episode #: 512\n",
      "episode #: 513\n",
      "episode #: 514\n",
      "episode #: 515\n",
      "episode #: 516\n",
      "episode #: 517\n",
      "episode #: 518\n",
      "episode #: 519\n",
      "episode #: 520\n",
      "episode #: 521\n",
      "episode #: 522\n",
      "episode #: 523\n",
      "episode #: 524\n",
      "episode #: 525\n",
      "episode #: 526\n",
      "episode #: 527\n",
      "episode #: 528\n",
      "episode #: 529\n",
      "episode #: 530\n",
      "episode #: 531\n",
      "episode #: 532\n",
      "episode #: 533\n",
      "episode #: 534\n",
      "episode #: 535\n",
      "episode #: 536\n",
      "episode #: 537\n",
      "episode #: 538\n",
      "episode #: 539\n",
      "episode #: 540\n",
      "episode #: 541\n",
      "episode #: 542\n",
      "episode #: 543\n",
      "episode #: 544\n",
      "episode #: 545\n",
      "episode #: 546\n",
      "episode #: 547\n",
      "episode #: 548\n",
      "episode #: 549\n",
      "episode #: 550\n",
      "episode #: 551\n",
      "episode #: 552\n",
      "episode #: 553\n",
      "episode #: 554\n",
      "episode #: 555\n",
      "episode #: 556\n",
      "episode #: 557\n",
      "episode #: 558\n",
      "episode #: 559\n",
      "episode #: 560\n",
      "episode #: 561\n",
      "episode #: 562\n",
      "episode #: 563\n",
      "episode #: 564\n",
      "episode #: 565\n",
      "episode #: 566\n",
      "episode #: 567\n",
      "episode #: 568\n",
      "episode #: 569\n",
      "episode #: 570\n",
      "episode #: 571\n",
      "episode #: 572\n",
      "episode #: 573\n",
      "episode #: 574\n",
      "episode #: 575\n",
      "episode #: 576\n",
      "episode #: 577\n",
      "episode #: 578\n",
      "episode #: 579\n",
      "episode #: 580\n",
      "episode #: 581\n",
      "episode #: 582\n",
      "episode #: 583\n",
      "episode #: 584\n",
      "episode #: 585\n",
      "episode #: 586\n",
      "episode #: 587\n",
      "episode #: 588\n",
      "episode #: 589\n",
      "episode #: 590\n",
      "episode #: 591\n",
      "episode #: 592\n",
      "episode #: 593\n",
      "episode #: 594\n",
      "episode #: 595\n",
      "episode #: 596\n",
      "episode #: 597\n",
      "episode #: 598\n",
      "episode #: 599\n",
      "episode #: 600\n",
      "episode #: 601\n",
      "episode #: 602\n",
      "episode #: 603\n",
      "episode #: 604\n",
      "episode #: 605\n",
      "episode #: 606\n",
      "episode #: 607\n",
      "episode #: 608\n",
      "episode #: 609\n",
      "episode #: 610\n",
      "episode #: 611\n",
      "episode #: 612\n",
      "episode #: 613\n",
      "episode #: 614\n",
      "episode #: 615\n",
      "episode #: 616\n",
      "episode #: 617\n",
      "episode #: 618\n",
      "episode #: 619\n",
      "episode #: 620\n",
      "episode #: 621\n",
      "episode #: 622\n",
      "episode #: 623\n",
      "episode #: 624\n",
      "episode #: 625\n",
      "episode #: 626\n",
      "episode #: 627\n",
      "episode #: 628\n",
      "episode #: 629\n",
      "episode #: 630\n",
      "episode #: 631\n",
      "episode #: 632\n",
      "episode #: 633\n",
      "episode #: 634\n",
      "episode #: 635\n",
      "episode #: 636\n",
      "episode #: 637\n",
      "episode #: 638\n",
      "episode #: 639\n",
      "episode #: 640\n",
      "episode #: 641\n",
      "episode #: 642\n",
      "episode #: 643\n",
      "episode #: 644\n",
      "episode #: 645\n",
      "episode #: 646\n",
      "episode #: 647\n",
      "episode #: 648\n",
      "episode #: 649\n",
      "episode #: 650\n",
      "episode #: 651\n",
      "episode #: 652\n",
      "episode #: 653\n",
      "episode #: 654\n",
      "episode #: 655\n",
      "episode #: 656\n",
      "episode #: 657\n",
      "episode #: 658\n",
      "episode #: 659\n",
      "episode #: 660\n",
      "episode #: 661\n",
      "episode #: 662\n",
      "episode #: 663\n",
      "episode #: 664\n",
      "episode #: 665\n",
      "episode #: 666\n",
      "episode #: 667\n",
      "episode #: 668\n",
      "episode #: 669\n",
      "episode #: 670\n",
      "episode #: 671\n",
      "episode #: 672\n",
      "episode #: 673\n",
      "episode #: 674\n",
      "episode #: 675\n",
      "episode #: 676\n",
      "episode #: 677\n",
      "episode #: 678\n",
      "episode #: 679\n",
      "episode #: 680\n",
      "episode #: 681\n",
      "episode #: 682\n",
      "episode #: 683\n",
      "episode #: 684\n",
      "episode #: 685\n",
      "episode #: 686\n",
      "episode #: 687\n",
      "episode #: 688\n",
      "episode #: 689\n",
      "episode #: 690\n",
      "episode #: 691\n",
      "episode #: 692\n",
      "episode #: 693\n",
      "episode #: 694\n",
      "episode #: 695\n",
      "episode #: 696\n",
      "episode #: 697\n",
      "episode #: 698\n",
      "episode #: 699\n",
      "episode #: 700\n",
      "episode #: 701\n",
      "episode #: 702\n",
      "episode #: 703\n",
      "episode #: 704\n",
      "episode #: 705\n",
      "episode #: 706\n",
      "episode #: 707\n",
      "episode #: 708\n",
      "episode #: 709\n",
      "episode #: 710\n",
      "episode #: 711\n",
      "episode #: 712\n",
      "episode #: 713\n",
      "episode #: 714\n",
      "episode #: 715\n",
      "episode #: 716\n",
      "episode #: 717\n",
      "episode #: 718\n",
      "episode #: 719\n",
      "episode #: 720\n",
      "episode #: 721\n",
      "episode #: 722\n",
      "episode #: 723\n",
      "episode #: 724\n",
      "episode #: 725\n",
      "episode #: 726\n",
      "episode #: 727\n",
      "episode #: 728\n",
      "episode #: 729\n",
      "episode #: 730\n",
      "episode #: 731\n",
      "episode #: 732\n",
      "episode #: 733\n",
      "episode #: 734\n",
      "episode #: 735\n",
      "episode #: 736\n",
      "episode #: 737\n",
      "episode #: 738\n",
      "episode #: 739\n",
      "episode #: 740\n",
      "episode #: 741\n",
      "episode #: 742\n",
      "episode #: 743\n",
      "episode #: 744\n",
      "episode #: 745\n",
      "episode #: 746\n",
      "episode #: 747\n",
      "episode #: 748\n",
      "episode #: 749\n",
      "episode #: 750\n",
      "episode #: 751\n",
      "episode #: 752\n",
      "episode #: 753\n",
      "episode #: 754\n",
      "episode #: 755\n",
      "episode #: 756\n",
      "episode #: 757\n",
      "episode #: 758\n",
      "episode #: 759\n",
      "episode #: 760\n",
      "episode #: 761\n",
      "episode #: 762\n",
      "episode #: 763\n",
      "episode #: 764\n",
      "episode #: 765\n",
      "episode #: 766\n",
      "episode #: 767\n",
      "episode #: 768\n",
      "episode #: 769\n",
      "episode #: 770\n",
      "episode #: 771\n",
      "episode #: 772\n",
      "episode #: 773\n",
      "episode #: 774\n",
      "episode #: 775\n",
      "episode #: 776\n",
      "episode #: 777\n",
      "episode #: 778\n",
      "episode #: 779\n",
      "episode #: 780\n",
      "episode #: 781\n",
      "episode #: 782\n",
      "episode #: 783\n",
      "episode #: 784\n",
      "episode #: 785\n",
      "episode #: 786\n",
      "episode #: 787\n",
      "episode #: 788\n",
      "episode #: 789\n",
      "episode #: 790\n",
      "episode #: 791\n",
      "episode #: 792\n",
      "episode #: 793\n",
      "episode #: 794\n",
      "episode #: 795\n",
      "episode #: 796\n",
      "episode #: 797\n",
      "episode #: 798\n",
      "episode #: 799\n",
      "episode #: 800\n",
      "episode #: 801\n",
      "episode #: 802\n",
      "episode #: 803\n",
      "episode #: 804\n",
      "episode #: 805\n",
      "episode #: 806\n",
      "episode #: 807\n",
      "episode #: 808\n",
      "episode #: 809\n",
      "episode #: 810\n",
      "episode #: 811\n",
      "episode #: 812\n",
      "episode #: 813\n",
      "episode #: 814\n",
      "episode #: 815\n",
      "episode #: 816\n",
      "episode #: 817\n",
      "episode #: 818\n",
      "episode #: 819\n",
      "episode #: 820\n",
      "episode #: 821\n",
      "episode #: 822\n",
      "episode #: 823\n",
      "episode #: 824\n",
      "episode #: 825\n",
      "episode #: 826\n",
      "episode #: 827\n",
      "episode #: 828\n",
      "episode #: 829\n",
      "episode #: 830\n",
      "episode #: 831\n",
      "episode #: 832\n",
      "episode #: 833\n",
      "episode #: 834\n",
      "episode #: 835\n",
      "episode #: 836\n",
      "episode #: 837\n",
      "episode #: 838\n",
      "episode #: 839\n",
      "episode #: 840\n",
      "episode #: 841\n",
      "episode #: 842\n",
      "episode #: 843\n",
      "episode #: 844\n",
      "episode #: 845\n",
      "episode #: 846\n",
      "episode #: 847\n",
      "episode #: 848\n",
      "episode #: 849\n",
      "episode #: 850\n",
      "episode #: 851\n",
      "episode #: 852\n",
      "episode #: 853\n",
      "episode #: 854\n",
      "episode #: 855\n",
      "episode #: 856\n",
      "episode #: 857\n",
      "episode #: 858\n",
      "episode #: 859\n",
      "episode #: 860\n",
      "episode #: 861\n",
      "episode #: 862\n",
      "episode #: 863\n",
      "episode #: 864\n",
      "episode #: 865\n",
      "episode #: 866\n",
      "episode #: 867\n",
      "episode #: 868\n",
      "episode #: 869\n",
      "episode #: 870\n",
      "episode #: 871\n",
      "episode #: 872\n",
      "episode #: 873\n",
      "episode #: 874\n",
      "episode #: 875\n",
      "episode #: 876\n",
      "episode #: 877\n",
      "episode #: 878\n",
      "episode #: 879\n",
      "episode #: 880\n",
      "episode #: 881\n",
      "episode #: 882\n",
      "episode #: 883\n",
      "episode #: 884\n",
      "episode #: 885\n",
      "episode #: 886\n",
      "episode #: 887\n",
      "episode #: 888\n",
      "episode #: 889\n",
      "episode #: 890\n",
      "episode #: 891\n",
      "episode #: 892\n",
      "episode #: 893\n",
      "episode #: 894\n",
      "episode #: 895\n",
      "episode #: 896\n",
      "episode #: 897\n",
      "episode #: 898\n",
      "episode #: 899\n",
      "episode #: 900\n",
      "episode #: 901\n",
      "episode #: 902\n",
      "episode #: 903\n",
      "episode #: 904\n",
      "episode #: 905\n",
      "episode #: 906\n",
      "episode #: 907\n",
      "episode #: 908\n",
      "episode #: 909\n",
      "episode #: 910\n",
      "episode #: 911\n",
      "episode #: 912\n",
      "episode #: 913\n",
      "episode #: 914\n",
      "episode #: 915\n",
      "episode #: 916\n",
      "episode #: 917\n",
      "episode #: 918\n",
      "episode #: 919\n",
      "episode #: 920\n",
      "episode #: 921\n",
      "episode #: 922\n",
      "episode #: 923\n",
      "episode #: 924\n",
      "episode #: 925\n",
      "episode #: 926\n",
      "episode #: 927\n",
      "episode #: 928\n",
      "episode #: 929\n",
      "episode #: 930\n",
      "episode #: 931\n",
      "episode #: 932\n",
      "episode #: 933\n",
      "episode #: 934\n",
      "episode #: 935\n",
      "episode #: 936\n",
      "episode #: 937\n",
      "episode #: 938\n",
      "episode #: 939\n",
      "episode #: 940\n",
      "episode #: 941\n",
      "episode #: 942\n",
      "episode #: 943\n",
      "episode #: 944\n",
      "episode #: 945\n",
      "episode #: 946\n",
      "episode #: 947\n",
      "episode #: 948\n",
      "episode #: 949\n",
      "episode #: 950\n",
      "episode #: 951\n",
      "episode #: 952\n",
      "episode #: 953\n",
      "episode #: 954\n",
      "episode #: 955\n",
      "episode #: 956\n",
      "episode #: 957\n",
      "episode #: 958\n",
      "episode #: 959\n",
      "episode #: 960\n",
      "episode #: 961\n",
      "episode #: 962\n",
      "episode #: 963\n",
      "episode #: 964\n",
      "episode #: 965\n",
      "episode #: 966\n",
      "episode #: 967\n",
      "episode #: 968\n",
      "episode #: 969\n",
      "episode #: 970\n",
      "episode #: 971\n",
      "episode #: 972\n",
      "episode #: 973\n",
      "episode #: 974\n",
      "episode #: 975\n",
      "episode #: 976\n",
      "episode #: 977\n",
      "episode #: 978\n",
      "episode #: 979\n",
      "episode #: 980\n",
      "episode #: 981\n",
      "episode #: 982\n",
      "episode #: 983\n",
      "episode #: 984\n",
      "episode #: 985\n",
      "episode #: 986\n",
      "episode #: 987\n",
      "episode #: 988\n",
      "episode #: 989\n",
      "episode #: 990\n",
      "episode #: 991\n",
      "episode #: 992\n",
      "episode #: 993\n",
      "episode #: 994\n",
      "episode #: 995\n",
      "episode #: 996\n",
      "episode #: 997\n",
      "episode #: 998\n",
      "episode #: 999\n"
     ]
    }
   ],
   "source": [
    "number_of_episodes = 1000\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    #if episode%10 == 0:\n",
    "    print(\"episode #:\", episode)\n",
    "    entity1.take_a_walk()\n",
    "    \n",
    "state_action_values1 = entity1.give_state_action_values()\n",
    "\n",
    "policy = np.zeros((dims[0],dims[1],4))\n",
    "\n",
    "for key in state_action_values1:\n",
    "    i =  key[0]\n",
    "    j = key[1]\n",
    "    policy[i,j] = state_action_values1[key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [-4.9999950960142705,\n",
       "  -4.9999950960142705,\n",
       "  -4.992262874754468,\n",
       "  -4.998701925785369],\n",
       " (0, 1): [-4.99999042190287,\n",
       "  -4.999963462459069,\n",
       "  -4.999976615973804,\n",
       "  -4.999981292779044],\n",
       " (0, 2): [-21.585832007585662,\n",
       "  -18.279817946754473,\n",
       "  -4.656402616320001,\n",
       "  -25.75685303984286],\n",
       " (0, 3): [-4.999999998726853,\n",
       "  -4.999999784320428,\n",
       "  -4.999999784320428,\n",
       "  -4.570503270400001],\n",
       " (0, 4): [-13.50149994487955,\n",
       "  -4.999999861965074,\n",
       "  -44.23980856115932,\n",
       "  -4.999999827456342],\n",
       " (0, 5): [-4.999999784320428,\n",
       "  -4.999999827456342,\n",
       "  -4.780097674444801,\n",
       "  -25.76180479993002],\n",
       " (0, 6): [-4.999996861449134,\n",
       "  -4.9999950960142705,\n",
       "  -4.4631290880000005,\n",
       "  -21.603254139803585],\n",
       " (0, 7): [-4.570503270400001,\n",
       "  -18.268665606068534,\n",
       "  -4.824078139555841,\n",
       "  -4.328911360000001],\n",
       " (0, 8): [-4.4631290880000005,\n",
       "  -4.887410009315738,\n",
       "  -4.887410009315738,\n",
       "  -4.725122093056001],\n",
       " (0, 9): [-4.942353924769659,\n",
       "  -3.6892800000000006,\n",
       "  -4.942353924769659,\n",
       "  -3.3616000000000006],\n",
       " (1, 0): [-13.485145780148532,\n",
       "  -6.195040899829781,\n",
       "  -5.122549933907091,\n",
       "  -4.999999999999966],\n",
       " (1, 1): [-45.54973538600211,\n",
       "  -4.999954328073834,\n",
       "  -4.999963462459068,\n",
       "  -4.999999341798178],\n",
       " (1, 2): [-8.44635934864695,\n",
       "  -4.725122093056001,\n",
       "  -15.782898855094652,\n",
       "  -84.12794240596207],\n",
       " (1, 3): [-15.630622285686933,\n",
       "  -45.49275392476967,\n",
       "  -4.4631290880000005,\n",
       "  -25.73819296758566],\n",
       " (1, 4): [-4.999999827456342,\n",
       "  -4.999999827456342,\n",
       "  -4.328911360000001,\n",
       "  -84.19978221928517],\n",
       " (1, 5): [-108.91031079701924,\n",
       "  -4.161139200000001,\n",
       "  -45.52678816758565,\n",
       "  -55.68757464704136],\n",
       " (1, 6): [-4.999976615973804,\n",
       "  -11.793556790307088,\n",
       "  -3.361600000000003,\n",
       "  -4.999942910092292],\n",
       " (1, 7): [-4.999888496274011,\n",
       "  -4.999727774106464,\n",
       "  -2.9520000000000004,\n",
       "  -55.687335386002125],\n",
       " (1, 8): [-4.990328593443085,\n",
       "  -3.9514240000000007,\n",
       "  -2.4400000000000004,\n",
       "  -4.725122093056001],\n",
       " (1, 9): [-4.780097674444801, -4.780097674444801, -3.6892800000000006, -1.8],\n",
       " (2, 0): [-45.49275392476967,\n",
       "  -21.585832007585662,\n",
       "  -4.859262511644673,\n",
       "  -45.5503923375223],\n",
       " (2, 1): [-4.999999911657648,\n",
       "  -11.803228189275455,\n",
       "  -4.824078139555841,\n",
       "  -103.7800976744448],\n",
       " (2, 2): [-84.19997661597381,\n",
       "  -4.780097674444801,\n",
       "  -4.999999827456342,\n",
       "  -103.92794240596207],\n",
       " (2, 3): [-84.08741000931575,\n",
       "  -4.999999827456342,\n",
       "  -84.18488842725482,\n",
       "  -168.09049106530952],\n",
       " (2, 4): [-4.999999998981483,\n",
       "  -4.99999042190287,\n",
       "  -68.32310651185259,\n",
       "  -103.99999999999997],\n",
       " (2, 5): [-84.19999966300068,\n",
       "  -4.999976615973804,\n",
       "  -84.19991079701921,\n",
       "  -103.99896154062829],\n",
       " (2, 6): [-4.999970769967254,\n",
       "  -45.54421029980358,\n",
       "  -4.999993870017837,\n",
       "  -103.97638816758565],\n",
       " (2, 7): [-4.999993870017837,\n",
       "  -55.65848520948207,\n",
       "  -84.08741000931575,\n",
       "  -129.11347908601002],\n",
       " (2, 8): [-2.9520000000000004, -3.6892800000000006, -1.8, -103.99683087349943],\n",
       " (2, 9): [-3.3616000000000006, -3.6892800000000006, -1.8, -1.0],\n",
       " (3, 0): [-84.19999973040053,\n",
       "  -4.999999911657648,\n",
       "  -107.48348989469835,\n",
       "  -39.45632306345083],\n",
       " (3, 1): [0, 0, 0, 0],\n",
       " (3, 2): [0, 0, 0, 0],\n",
       " (3, 3): [0, 0, 0, 0],\n",
       " (3, 4): [0, 0, 0, 0],\n",
       " (3, 5): [0, 0, 0, 0],\n",
       " (3, 6): [0, 0, 0, 0],\n",
       " (3, 7): [0, 0, 0, 0],\n",
       " (3, 8): [0, 0, 0, 0],\n",
       " (3, 9): [0, 0, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing policy walk\n",
      "current state is: [3, 0]\n",
      "current step is: 0\n",
      "next action is: up\n",
      "current state is: [2, 0]\n",
      "current step is: 1\n",
      "next action is: right\n",
      "current state is: [2, 1]\n",
      "current step is: 2\n",
      "next action is: right\n",
      "current state is: [2, 2]\n",
      "current step is: 3\n",
      "next action is: up\n",
      "current state is: [1, 2]\n",
      "current step is: 4\n",
      "next action is: up\n",
      "current state is: [0, 2]\n",
      "current step is: 5\n",
      "next action is: right\n",
      "current state is: [0, 3]\n",
      "current step is: 6\n",
      "next action is: down\n",
      "current state is: [1, 3]\n",
      "current step is: 7\n",
      "next action is: right\n",
      "current state is: [1, 4]\n",
      "current step is: 8\n",
      "next action is: right\n",
      "current state is: [1, 5]\n",
      "current step is: 9\n",
      "next action is: up\n",
      "current state is: [0, 5]\n",
      "current step is: 10\n",
      "next action is: right\n",
      "current state is: [0, 6]\n",
      "current step is: 11\n",
      "next action is: right\n",
      "current state is: [0, 7]\n",
      "current step is: 12\n",
      "next action is: down\n",
      "current state is: [1, 7]\n",
      "current step is: 13\n",
      "next action is: right\n",
      "current state is: [1, 8]\n",
      "current step is: 14\n",
      "next action is: right\n",
      "current state is: [1, 9]\n",
      "current step is: 15\n",
      "next action is: down\n",
      "current state is: [2, 9]\n",
      "current step is: 16\n",
      "next action is: down\n",
      "current state is: [3, 9]\n",
      "current step is: 17\n",
      "next action is: left\n",
      "you have reached the terminal state in  18  steps!\n"
     ]
    }
   ],
   "source": [
    "#optimal steps for this problem is 15, book notes 17 is the average on their solution\n",
    "#this method sets epsilon to 0 and uses the q values to do a pure exploitation.\n",
    "print(\"doing policy walk\")\n",
    "entity1.policy_walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arbitrary state_action_values\n",
    "#using a dict because look up is much easier then\n",
    "state_action_values2 = {(None,None,None):0}\n",
    "\n",
    "for i in range(dims[0]):\n",
    "    for j in range(dims[1]):\n",
    "        state_action_values2[(i,j)] = [0,0,0,0]\n",
    "        \n",
    "state_action_values2.pop((None,None,None))\n",
    "            \n",
    "#state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment2 = environment()\n",
    "entity2 = q_entity(state_action_values2, environment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_episodes = 1000\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    #if episode%100 == 0:\n",
    "        #print(\"episode #:\", episode)\n",
    "    entity2.take_a_walk()\n",
    "    \n",
    "state_action_values2 = entity2.give_state_action_values()\n",
    "\n",
    "policy = np.zeros((dims[0],dims[1],4))\n",
    "\n",
    "for key in state_action_values2:\n",
    "    i =  key[0]\n",
    "    j = key[1]\n",
    "    policy[i,j] = state_action_values2[key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [-1.9982772163948412,\n",
       "  -1.9982804034033264,\n",
       "  -1.9981655240228418,\n",
       "  -1.9980920541599498],\n",
       " (0, 1): [-1.9975229906714769,\n",
       "  -1.9974124735669276,\n",
       "  -1.9972413547629473,\n",
       "  -1.9973167329666353],\n",
       " (0, 2): [-1.995852982947096,\n",
       "  -1.995525026210022,\n",
       "  -1.995367098949223,\n",
       "  -1.9956174437046907],\n",
       " (0, 3): [-1.9931777710896346,\n",
       "  -1.992612581126578,\n",
       "  -1.992052402063783,\n",
       "  -1.9921307943278552],\n",
       " (0, 4): [-1.987563763935484,\n",
       "  -1.9872598134768582,\n",
       "  -1.985880438121313,\n",
       "  -1.986207040951853],\n",
       " (0, 5): [-1.9746191937931403,\n",
       "  -1.975357575556639,\n",
       "  -1.9742151659700995,\n",
       "  -1.9765628037383902],\n",
       " (0, 6): [-1.9535827353802075,\n",
       "  -1.958383693508491,\n",
       "  -1.9545606044920825,\n",
       "  -1.9564248616963713],\n",
       " (0, 7): [-1.9173067503607701,\n",
       "  -1.9273667011873585,\n",
       "  -1.9194572859678656,\n",
       "  -1.918039633373164],\n",
       " (0, 8): [-1.861850617605672,\n",
       "  -1.8615824824521396,\n",
       "  -1.8564796027934158,\n",
       "  -1.8572481031197396],\n",
       " (0, 9): [-1.757305664113002,\n",
       "  -1.7631773881323056,\n",
       "  -1.7611487116806046,\n",
       "  -1.7429170616356398],\n",
       " (1, 0): [-1.998623383520181,\n",
       "  -1.9986279700580416,\n",
       "  -1.9985579772095265,\n",
       "  -1.9986307441964741],\n",
       " (1, 1): [-1.9975985852271094,\n",
       "  -1.9977826754208787,\n",
       "  -1.9975860802127854,\n",
       "  -1.9976459657293293],\n",
       " (1, 2): [-1.9958498077981566,\n",
       "  -1.9959789148229476,\n",
       "  -1.9957573839416214,\n",
       "  -1.9957554157858843],\n",
       " (1, 3): [-1.9925015778261774,\n",
       "  -1.9922079482714703,\n",
       "  -1.9918855615161288,\n",
       "  -1.9918675217942723],\n",
       " (1, 4): [-1.985543715712076,\n",
       "  -1.985899670099639,\n",
       "  -1.984082718328831,\n",
       "  -1.984094223644888],\n",
       " (1, 5): [-1.9729537886523976,\n",
       "  -1.9699928297163083,\n",
       "  -1.9685856276068308,\n",
       "  -1.96861608384321],\n",
       " (1, 6): [-1.9454192519527744,\n",
       "  -1.9537101138338897,\n",
       "  -1.9373865301215412,\n",
       "  -1.93740697718644],\n",
       " (1, 7): [-1.934426613407494,\n",
       "  -1.8798565621407963,\n",
       "  -1.8749475606797839,\n",
       "  -1.8749479891317429],\n",
       " (1, 8): [-1.8224631098518933,\n",
       "  -1.8054926830170197,\n",
       "  -1.749982286686403,\n",
       "  -1.749982109199473],\n",
       " (1, 9): [-1.550275824096801,\n",
       "  -1.6052909439619132,\n",
       "  -1.5870357957317025,\n",
       "  -1.4999999585368609],\n",
       " (2, 0): [-1.9990135754600173,\n",
       "  -1.999194673372315,\n",
       "  -1.9980468749999996,\n",
       "  -1.9995056314504007],\n",
       " (2, 1): [-1.9990132457198508,\n",
       "  -1.9985538461642176,\n",
       "  -1.9960937499999996,\n",
       "  -100.94235763074597],\n",
       " (2, 2): [-1.9980433415083514,\n",
       "  -1.9976532125550004,\n",
       "  -1.9921874999999996,\n",
       "  -100.75856816720167],\n",
       " (2, 3): [-1.9958672563578896,\n",
       "  -1.995488950496693,\n",
       "  -1.9843749999999996,\n",
       "  -100.81904455936035],\n",
       " (2, 4): [-1.9920390275170332,\n",
       "  -1.988140105517641,\n",
       "  -1.9687499999999996,\n",
       "  -99.64757395665387],\n",
       " (2, 5): [-1.9842478150275131,\n",
       "  -1.9841060155753625,\n",
       "  -1.9374999999999996,\n",
       "  -100.81898478995916],\n",
       " (2, 6): [-1.962646058508223,\n",
       "  -1.9648453516336764,\n",
       "  -1.8749999999999996,\n",
       "  -100.42899925303705],\n",
       " (2, 7): [-1.9370264334796359,\n",
       "  -1.9364974026911672,\n",
       "  -1.7499999999999996,\n",
       "  -100.23965565360461],\n",
       " (2, 8): [-1.872893144120427,\n",
       "  -1.8723060523292752,\n",
       "  -1.4999999999999996,\n",
       "  -100.23955164100931],\n",
       " (2, 9): [-1.7487830063526124,\n",
       "  -1.678801270237658,\n",
       "  -1.4905543339375664,\n",
       "  -0.9999999999999998],\n",
       " (3, 0): [-1.9995104341004992,\n",
       "  -1.9990234374999996,\n",
       "  -100.81848033921703,\n",
       "  -1.999507573942354],\n",
       " (3, 1): [0, 0, 0, 0],\n",
       " (3, 2): [0, 0, 0, 0],\n",
       " (3, 3): [0, 0, 0, 0],\n",
       " (3, 4): [0, 0, 0, 0],\n",
       " (3, 5): [0, 0, 0, 0],\n",
       " (3, 6): [0, 0, 0, 0],\n",
       " (3, 7): [0, 0, 0, 0],\n",
       " (3, 8): [0, 0, 0, 0],\n",
       " (3, 9): [0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing policy walk\n",
      "current state is: [3, 0]\n",
      "current step is: 0\n",
      "next action is: up\n",
      "current state is: [2, 0]\n",
      "current step is: 1\n",
      "next action is: right\n",
      "current state is: [2, 1]\n",
      "current step is: 2\n",
      "next action is: right\n",
      "current state is: [2, 2]\n",
      "current step is: 3\n",
      "next action is: right\n",
      "current state is: [2, 3]\n",
      "current step is: 4\n",
      "next action is: right\n",
      "current state is: [2, 4]\n",
      "current step is: 5\n",
      "next action is: right\n",
      "current state is: [2, 5]\n",
      "current step is: 6\n",
      "next action is: right\n",
      "current state is: [2, 6]\n",
      "current step is: 7\n",
      "next action is: right\n",
      "current state is: [2, 7]\n",
      "current step is: 8\n",
      "next action is: right\n",
      "current state is: [2, 8]\n",
      "current step is: 9\n",
      "next action is: right\n",
      "current state is: [2, 9]\n",
      "current step is: 10\n",
      "next action is: down\n",
      "current state is: [3, 9]\n",
      "current step is: 11\n",
      "next action is: left\n",
      "you have reached the terminal state in  12  steps!\n"
     ]
    }
   ],
   "source": [
    "#optimal steps for this problem is 15, book notes 17 is the average on their solution\n",
    "#this method sets epsilon to 0 and uses the q values to do a pure exploitation.\n",
    "print(\"doing policy walk\")\n",
    "entity2.policy_walk()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
