{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k arm bandit \n",
    "\n",
    "this notebook is an attmpt to create a k armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables\n",
    "k = 10\n",
    "reward_range_min = 1\n",
    "reward_range_max = 10\n",
    "std_min = 1\n",
    "std_max = 1\n",
    "samples_per_distrib = 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [10]\n",
      "1 [3]\n",
      "2 [9]\n",
      "3 [5]\n",
      "4 [11]\n",
      "5 [3]\n",
      "6 [9]\n",
      "7 [9]\n",
      "8 [8]\n",
      "9 [1]\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n"
     ]
    }
   ],
   "source": [
    "#create arms and rewards\n",
    "reward_stack = []\n",
    "max_mean = 0\n",
    "best_arm = 0\n",
    "\n",
    "for i in range(0,k):\n",
    "    #select mean for this arm\n",
    "    current_mean = np.random.choice(a = list(range(reward_range_min,reward_range_max+1)), size = 1)\n",
    "    \n",
    "    if max_mean == current_mean:\n",
    "        current_mean += 1 \n",
    "    \n",
    "    if max_mean < current_mean:\n",
    "        max_mean = current_mean\n",
    "        best_arm = i\n",
    "        \n",
    "    #to debug\n",
    "    print(i , current_mean)\n",
    "    \n",
    "    #select std for this arm\n",
    "    current_std = np.random.choice(a = list(range(std_min,std_max+1)), size = 1)\n",
    "    #to debug\n",
    "    #print(current_std)\n",
    "    \n",
    "    #draw samples for this arm\n",
    "    current_arm = np.random.normal(loc = current_mean, scale = current_std, size = samples_per_distrib)\n",
    "    #to debug\n",
    "    #print(current_arm)\n",
    "    \n",
    "    #append to reward stack list \n",
    "    reward_stack.append(current_arm)\n",
    "    #to debug\n",
    "    #print(reward_stack)\n",
    "    \n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bodies': [<matplotlib.collections.PolyCollection at 0x7f3f61f939a0>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61f932b0>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61f88850>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61f972e0>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61f97f10>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61fb3250>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61fb3550>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61fb3850>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61fb3b50>,\n",
       "  <matplotlib.collections.PolyCollection at 0x7f3f61fb3e50>],\n",
       " 'cmeans': <matplotlib.collections.LineCollection at 0x7f3f61f93e80>,\n",
       " 'cmaxes': <matplotlib.collections.LineCollection at 0x7f3f61f3e4c0>,\n",
       " 'cmins': <matplotlib.collections.LineCollection at 0x7f3f61f3e970>,\n",
       " 'cbars': <matplotlib.collections.LineCollection at 0x7f3f61f3ee20>,\n",
       " 'cmedians': <matplotlib.collections.LineCollection at 0x7f3f61f58310>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#alter plot size so it looks decent\n",
    "#makes viz bigger\n",
    "import matplotlib.pyplot as plt\n",
    "BIG = 16\n",
    "MED = 14\n",
    "SMALL = 10\n",
    "plt.rcParams.update({'font.size': BIG})\n",
    "plt.rcParams.update({'axes.labelsize': BIG})\n",
    "plt.rcParams.update({'xtick.labelsize': MED})\n",
    "plt.rcParams.update({'ytick.labelsize': MED})\n",
    "plt.rcParams.update({'figure.figsize':[12, 8]}) #this is important as it makes the visualizations much easier to read and clip/export!(w,h)\n",
    "\n",
    "#plot reward stack and arms\n",
    "plt.xticks(ticks=list(range(1,k+1)), labels=list(range(1,k+1)))\n",
    "\n",
    "plt.xlabel(\"bandit arm\")\n",
    "\n",
    "plt.ylabel(\"values\")\n",
    "\n",
    "plt.violinplot(reward_stack, list(range(0,k)), points=20, widths=0.3,\n",
    "                     showmeans=True, showextrema=True, showmedians=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward from random selection: 6738.241173278457\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "0    0.093\n",
      "1    0.100\n",
      "2    0.090\n",
      "3    0.099\n",
      "4    0.104\n",
      "5    0.102\n",
      "6    0.111\n",
      "7    0.091\n",
      "8    0.105\n",
      "9    0.105\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#random selection without any value assesments\n",
    "\n",
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set objects needed\n",
    "arms_selected = []\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(0,n):\n",
    "    \n",
    "    #randomly pick an arm\n",
    "    current_arm = np.random.choice(range(0,k))\n",
    "    \n",
    "    #append choice to list tracking what was done\n",
    "    arms_selected.append(current_arm)\n",
    "    \n",
    "    #randomly pick a reward from the distrib\n",
    "    current_reward = np.random.choice(reward_stack[current_arm])\n",
    "    \n",
    "    #add current reward to total rewards\n",
    "    total_reward = total_reward + current_reward\n",
    "    \n",
    "\n",
    "print(\"total reward from random selection:\",total_reward)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(pd.Series(arms_selected).value_counts(normalize = True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon greedy solution\n",
    "\n",
    "#example for setting base vars\n",
    "n = 1000\n",
    "reward_step = .9 #used with nonstationary tracking, set to one for stationary problems\n",
    "epsilon = 0.1\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "##############################################################################################\n",
    "#some basic helper functions\n",
    "##############################################################################################\n",
    "\n",
    "#use epsilon to make explouit or explore decision:\n",
    "def exploit_or_explore(epsilon):\n",
    "    \n",
    "    exp = np.random.binomial(1, epsilon, 1)\n",
    "    \n",
    "    return exp\n",
    "\n",
    "#value assesment\n",
    "def arm_max(expected_rewards):\n",
    "    \n",
    "    maxx = np.argsort(expected_rewards)[-1]\n",
    "    \n",
    "    #debug\n",
    "    #print(expected_rewards)\n",
    "    #print(maxx)\n",
    "    \n",
    "    return maxx\n",
    "    \n",
    "#pull the arm selected\n",
    "def pull_arm(current_arm):\n",
    "    \n",
    "    current_reward = np.random.choice(reward_stack[current_arm])\n",
    "    \n",
    "    return current_reward   \n",
    "\n",
    "##############################################################################################\n",
    "#some not so basic helper functions\n",
    "##############################################################################################\n",
    "\n",
    "#update reward expectations\n",
    "def update_reward_expectation(expected_rewards, last_reward_list, reward_step, current_arm):\n",
    "    \n",
    "    prior_tweaked = (reward_step * (last_reward_list[current_arm] - expected_rewards[current_arm]))\n",
    "    \n",
    "    #debug\n",
    "    #print(prior_tweaked)\n",
    "    \n",
    "    expected_rewards[current_arm] = expected_rewards[current_arm] + prior_tweaked\n",
    "    \n",
    "    #print(expected_rewards)\n",
    "    \n",
    "    return expected_rewards\n",
    "\n",
    "#when epsilon is used for exploration\n",
    "#select arm that has been selected the least number of times or random arm (not selecting the max arm)\n",
    "def arm_explore(expected_rewards, count_arms_selected):\n",
    "         \n",
    "    #get the number of times each arm has been selected\n",
    "    arm_breakdown = np.array(count_arms_selected)\n",
    "    \n",
    "    #check if first two are equal\n",
    "    sorted_arm_indices = arm_breakdown.argsort()\n",
    "    \n",
    "    #debug\n",
    "    #print(arm_breakdown)\n",
    "    #print(sorted_arm_indices)\n",
    "    #print(\"0:\",arm_breakdown[sorted_arm_indices[0]])\n",
    "    #print(\"1:\",arm_breakdown[sorted_arm_indices[1]])\n",
    "    \n",
    "    #if first do not have equal counts then return lowest count\n",
    "    if arm_breakdown[sorted_arm_indices[0]] != arm_breakdown[sorted_arm_indices[1]]:\n",
    "        return sorted_arm_indices[0]\n",
    "            \n",
    "    #take random sample\n",
    "    max_arm = arm_max(expected_rewards)\n",
    "    \n",
    "    rando_list = [i for i in range(0,k)]\n",
    "    \n",
    "    rando_list.remove(max_arm)\n",
    "    \n",
    "    selected_element = np.random.choice(rando_list)\n",
    "    \n",
    "    return selected_element\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "#main calls\n",
    "##############################################################################################\n",
    "#play one turn\n",
    "def play_one(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step):\n",
    "    \n",
    "    if epsilon > 0:\n",
    "    \n",
    "        #get explore/exploit signal\n",
    "        exp = exploit_or_explore(epsilon)\n",
    "        \n",
    "    else:\n",
    "        exp = 0\n",
    "    \n",
    "    #explore\n",
    "    if exp:\n",
    "        current_arm = arm_explore(expected_rewards, count_arms_selected)\n",
    "    \n",
    "    #exploit\n",
    "    else:\n",
    "        current_arm = arm_max(expected_rewards)\n",
    "        \n",
    "        #debug\n",
    "        #print(current_arm)\n",
    "        \n",
    "    #pull the arm\n",
    "    current_reward = pull_arm(current_arm)\n",
    "    \n",
    "    #append reward to reward_list\n",
    "    last_reward_list[current_arm] = current_reward\n",
    "    \n",
    "    #append arms selected to tracking\n",
    "    count_arms_selected[current_arm] += 1  \n",
    "    \n",
    "    #print(current_arm)\n",
    "    \n",
    "    #update reward expectation\n",
    "    expected_rewards = update_reward_expectation(expected_rewards, last_reward_list, reward_step, current_arm)\n",
    "    \n",
    "    #update total reward\n",
    "    total_reward = total_reward + current_reward\n",
    "    \n",
    "    return expected_rewards, last_reward_list, count_arms_selected, total_reward\n",
    "\n",
    "##############################################################################################\n",
    "#play thru\n",
    "##############################################################################################\n",
    "def play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first):\n",
    "\n",
    "    start = 0\n",
    "    \n",
    "    if explore_first:\n",
    "        \n",
    "        start = k\n",
    "    \n",
    "        #for first k rounds pull each arm available once to get started    \n",
    "        for current_arm in range(0,k):\n",
    "            #debug\n",
    "            #print(i)\n",
    "            current_reward = pull_arm(current_arm)\n",
    "\n",
    "            #append arms selected to tracking\n",
    "            count_arms_selected[current_arm] += 1  \n",
    "\n",
    "            expected_rewards[current_arm] = current_reward\n",
    "\n",
    "            #update total reward\n",
    "            total_reward = total_reward + current_reward\n",
    "\n",
    "            #debug\n",
    "            #print(total_reward)\n",
    "     \n",
    "    #play thru remaining rounds\n",
    "    for i in range(start,n):\n",
    "        #debug\n",
    "        #print(i)\n",
    "    \n",
    "        #start the process using epsilon greedy\n",
    "        expected_rewards, last_reward_list, count_arms_selected, total_reward = play_one(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step)  \n",
    "        \n",
    "    return expected_rewards, last_reward_list, count_arms_selected, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create run tracking\n",
    "dummy_list = []\n",
    "columns = [\"explore first\", \"epsilon\", \"reward step\", \"total rewards\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  7830.042344134017 \n",
      "epsilon =  0 \n",
      "reward step =  1\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [  0   0   0   0   0   0   0   0 970  30]\n",
      "final expected rewards: [ 0  0  0  0  0  0  0  0  8 -1]\n",
      "final last rewards: [ 0  0  0  0  0  0  0  0  8 -1]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = 1 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0 so its totally greedy with no exploration\n",
    "epsilon = 0\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 0\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  10014.454955905498 \n",
      "epsilon =  0.1 \n",
      "reward step =  1\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [ 62  16  30  20 701  16  44  77  18  16]\n",
      "final expected rewards: [9 3 8 4 9 3 7 8 7 2]\n",
      "final last rewards: [9 3 8 4 9 3 7 8 7 2]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = 1 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0.1 so we eplore 10% of the time\n",
    "epsilon = 0.1\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 0\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  8618.151097059916 \n",
      "epsilon =  0.01 \n",
      "reward step =  1\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [  0   0   0   2   0   1 630   0 361   6]\n",
      "final expected rewards: [0 0 0 4 0 2 6 0 8 0]\n",
      "final last rewards: [0 0 0 4 0 2 6 0 8 0]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = 1 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0.01 so we eplore 1% of the time\n",
    "epsilon = 0.01\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 0\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  8972.927667362252 \n",
      "epsilon =  0.01 \n",
      "reward step =  0.9\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [172   1  19   2 245   1   5 445  35  75]\n",
      "final expected rewards: [8 3 6 5 9 3 6 6 5 0]\n",
      "final last rewards: [9 4 6 6 9 4 6 6 5 0]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = .9 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0.01 so we eplore 1% of the time\n",
    "epsilon = 0.01\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 0\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  10880.460806622701 \n",
      "epsilon =  0 \n",
      "reward step =  1\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [  2   1   1   1 977   1  14   1   1   1]\n",
      "final expected rewards: [ 8  2  8  6 10  4  7  6  7  2]\n",
      "final last rewards: [ 8  0  0  0 10  0  7  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = 1 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0 so its totally greedy with no exploration\n",
    "epsilon = 0\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 1\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  10193.977981064336 \n",
      "epsilon =  0.1 \n",
      "reward step =  1\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [ 31  12  33  12 735  15  68  58  24  12]\n",
      "final expected rewards: [ 7  2  8  6  8  2  8 10  7  1]\n",
      "final last rewards: [ 7  2  8  6  8  2  8 10  7  1]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = 1 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0.1 so we eplore 10% of the time\n",
    "epsilon = 0.1\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 1\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  10855.092547897522 \n",
      "epsilon =  0.01 \n",
      "reward step =  1\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [  8   1   2   1 961   2  12   2   9   2]\n",
      "final expected rewards: [ 8  4  7  4 10  3  6  6  7  0]\n",
      "final last rewards: [ 8  0  7  0 10  3  6  6  7  0]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = 1 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0.01 so we eplore 1% of the time\n",
    "epsilon = 0.01\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 1\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward =  10882.69557757302 \n",
      "epsilon =  0.01 \n",
      "reward step =  0.9\n",
      "the maximum mean is : [11]\n",
      "the best arm is : 4\n",
      "total times each arm selected: [  2   1   3   1 969   2   2  17   1   2]\n",
      "final expected rewards: [ 8  1  8  6 10  2  7  7  7  0]\n",
      "final last rewards: [ 9  0  8  0 11  3  7  7  0  1]\n"
     ]
    }
   ],
   "source": [
    "#set base vars\n",
    "n = 1000\n",
    "\n",
    "#set to one as this is stationary\n",
    "reward_step = .9 #used with nonstationary tracking, set to one for stationary problems\n",
    "\n",
    "#set to 0.01 so we eplore 1% of the time\n",
    "epsilon = 0.01\n",
    "\n",
    "#set explore first to 1 to have algo pull each arm once\n",
    "explore_first = 1\n",
    "\n",
    "#set objects needed\n",
    "count_arms_selected = np.array([0 for i in list(range(0,k))])\n",
    "last_reward_list = np.array([0 for i in list(range(0,k))])\n",
    "expected_rewards = np.array([0 for i in list(range(0,k))])\n",
    "total_reward = 0\n",
    "\n",
    "expected_rewards, last_reward_list, count_arms_selected, total_reward = play_the_bandit(expected_rewards, last_reward_list, count_arms_selected, total_reward, epsilon, reward_step, n, explore_first)\n",
    "\n",
    "print(\"total reward = \" ,total_reward, \"\\nepsilon = \", epsilon,\"\\nreward step = \", reward_step)\n",
    "print(\"the maximum mean is :\", max_mean)\n",
    "print(\"the best arm is :\", best_arm)  \n",
    "print(\"total times each arm selected:\", count_arms_selected)\n",
    "print(\"final expected rewards:\", expected_rewards)\n",
    "print(\"final last rewards:\", last_reward_list)\n",
    "\n",
    "dummy_list.append([explore_first, epsilon, reward_step, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explore first</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>reward step</th>\n",
       "      <th>total rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7830.042344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10014.454956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8618.151097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>8972.927667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10880.460807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10193.977981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10855.092548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10882.695578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   explore first  epsilon  reward step  total rewards\n",
       "0              0     0.00          1.0    7830.042344\n",
       "1              0     0.10          1.0   10014.454956\n",
       "2              0     0.01          1.0    8618.151097\n",
       "3              0     0.01          0.9    8972.927667\n",
       "4              1     0.00          1.0   10880.460807\n",
       "5              1     0.10          1.0   10193.977981\n",
       "6              1     0.01          1.0   10855.092548\n",
       "7              1     0.01          0.9   10882.695578"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data = dummy_list, columns = columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explore first</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>reward step</th>\n",
       "      <th>total rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10949.160049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10882.695578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10880.460807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10855.092548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10193.977981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10014.454956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>8972.927667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8618.151097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7830.042344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   explore first  epsilon  reward step  total rewards\n",
       "8             -1    -1.00         -1.0   10949.160049\n",
       "7              1     0.01          0.9   10882.695578\n",
       "4              1     0.00          1.0   10880.460807\n",
       "6              1     0.01          1.0   10855.092548\n",
       "5              1     0.10          1.0   10193.977981\n",
       "1              0     0.10          1.0   10014.454956\n",
       "3              0     0.01          0.9    8972.927667\n",
       "2              0     0.01          1.0    8618.151097\n",
       "0              0     0.00          1.0    7830.042344"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bayesian agent with thompson sampling\n",
    "#other congugate priors can be determined from the table of such in wikipedia here:https://en.wikipedia.org/wiki/Conjugate_prior\n",
    "num_levers = len(reward_stack)\n",
    "taus = [0.0001 for i in range(num_levers)]\n",
    "mews = [1 for i in range(num_levers)]\n",
    "actions = [0 for i in range(num_levers)]\n",
    "rewards = [0 for i in range(num_levers)]\n",
    "total_reward = 0\n",
    "\n",
    "n = 1000\n",
    "\n",
    "#pull the arm selected\n",
    "def pull_arm(current_arm):\n",
    "    \n",
    "    current_reward = np.random.choice(reward_stack[current_arm])\n",
    "    \n",
    "    return current_reward   \n",
    "\n",
    "def value_samples(taus, mews, num_levers):\n",
    "    return [(np.random.randn() / np.sqrt(taus[i])) + mews[i] for i in range(num_levers)]\n",
    "\n",
    "def choose_action(samples):\n",
    "    return np.argmax(samples)\n",
    "\n",
    "def update(taus, mews, rewards, action, actions):\n",
    "    taus[action] += (1 * actions[action]) #we know preceision is one here b/c variance is 1\n",
    "    mews[action] += ((mews[action] * taus[action]) + np.mean(rewards[action]))/(taus[action] + actions[action])\n",
    "    return taus, mews\n",
    "    \n",
    "for i in range(n):\n",
    "    samples = value_samples(taus, mews, num_levers)    \n",
    "    action = choose_action(samples)\n",
    "    actions[action] += 1\n",
    "    reward = pull_arm(action)  \n",
    "    rewards[action] +=  reward\n",
    "    taus, mews = update(taus, mews, rewards, action, actions)\n",
    "    total_reward += reward\n",
    "    \n",
    "dummy_list.append([-1, -1, -1, total_reward]) #-1 for all otehr variables to indicate bayesian\n",
    "\n",
    "df = pd.DataFrame(data = dummy_list, columns = columns)\n",
    "\n",
    "df.sort_values(by = ['total rewards'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
